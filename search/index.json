[{"content":"CLIP ä»£ç  1ã€CLIPVisionModel è°ƒç”¨ CLIPVisionTransformer\n1 2 3 4 5 6 7 8 9 class CLIPVisionModel(CLIPPreTrainedModel): def __init__(self, config: CLIPVisionConfig): super().__init__(config) self.vision_model = CLIPVisionTransformer(config) self.post_init() def forward(...): return self.vision_model(...) 2ã€CLIPVisionTransformer å®ä¾‹åŒ– CLIPVisionEmbeddings ä½œä¸º embeddings å±‚ï¼Œå¤„ç† pixel_values å¾—åˆ° hidden_states ä½¿ç”¨ layernorm è¿›è¡Œå¤„ç† CLIPEncoder å¤„ç†å¾—åˆ° encoder_outputsã€‚ last_hidden_stateï¼šæœ€åä¸€å±‚ layer çš„ hidden_states è¾“å‡ºï¼Œ(batch_size, sequence_length, hidden_size) hidden_statesï¼šæ‰€æœ‰ layer çš„ hidden_states è¾“å‡º attentionsï¼šæ‰€æœ‰ layer çš„ attn_weights è¾“å‡ºï¼Œ(batch_size, num_heads, sequence_length, sequence_length) å– [CLS] Token ä½œä¸º pooled_output 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class CLIPVisionTransformer(nn.Module): def __init__(self, config: CLIPVisionConfig): super().__init__() self.config = config embed_dim = config.hidden_size self.embeddings = CLIPVisionEmbeddings(config) self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps) self.encoder = CLIPEncoder(config) self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps) def forward(...): hidden_states = self.embeddings(pixel_values) hidden_states = self.pre_layrnorm(hidden_states) encoder_outputs = self.encoder( inputs_embeds=hidden_states, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, ) last_hidden_state = encoder_outputs[0] pooled_output = last_hidden_state[:, 0, :] pooled_output = self.post_layernorm(pooled_output) if not return_dict: return (last_hidden_state, pooled_output) + encoder_outputs[1:] return BaseModelOutputWithPooling( last_hidden_state=last_hidden_state, pooler_output=pooled_output, hidden_states=encoder_outputs.hidden_states, attentions=encoder_outputs.attentions, ) 3ã€CLIPVisionEmbeddings åˆå§‹åŒ– [CLS] embeddingã€patch_embedding å’Œ position_embeddingã€‚å…¶ä¸­ï¼Œpatch_embedding æ˜¯ä¸€ä¸ªäºŒç»´å·ç§¯ æ³¨å†Œä¸€ä¸ªç¼“å†²åŒºï¼Œç”¨äºå­˜å‚¨ä½ç½®ç¼–ç  å°† pixel_values ç» patch_embedding å¤„ç†ï¼Œå¾—åˆ° patch_embedsï¼Œå½¢çŠ¶ä¸º (batch_size, self.embed_dim, H_out, W_out) å…ˆä½¿ç”¨ flatten(2) å°†æœ€åä¸¤ä¸ªç»´åº¦å±•å¹³ï¼Œå˜ä¸º (batch_size, self.embed_dim, height * width)ï¼Œå†ä½¿ç”¨ transpose(1, 2) äº¤æ¢ç¬¬1å’Œç¬¬2ç»´åº¦ï¼Œæœ€ç»ˆå¾—åˆ° (batch_size, height * width, self.embed_dim) ä½¿ç”¨ expand(batch_size, 1, -1) å°† class_embedding æ‰©å±•ä¸º (batch_size, 1, self.embed_dim) è¿æ¥ class_embeds, patch_embedsï¼Œæ·»åŠ ä½ç½®ç¼–ç  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class CLIPVisionEmbeddings(nn.Module): def __init__(self, config: CLIPVisionConfig): super().__init__() self.class_embedding = nn.Parameter(torch.randn(self.embed_dim)) self.patch_embedding = nn.Conv2d( in_channels=config.num_channels, out_channels=self.embed_dim, kernel_size=self.patch_size, stride=self.patch_size, bias=False, ) self.num_patches = (self.image_size // self.patch_size) ** 2 self.num_positions = self.num_patches + 1 self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim) self.register_buffer(\u0026#34;position_ids\u0026#34;, torch.arange(self.num_positions).expand((1, -1)), persistent=False) def forward(self, pixel_values: torch.FloatTensor) -\u0026gt; torch.Tensor: batch_size = pixel_values.shape[0] target_dtype = self.patch_embedding.weight.dtype patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype)) # shape = [*, width, grid, grid] patch_embeds = patch_embeds.flatten(2).transpose(1, 2) class_embeds = self.class_embedding.expand(batch_size, 1, -1) embeddings = torch.cat([class_embeds, patch_embeds], dim=1) embeddings = embeddings + self.position_embedding(self.position_ids) return embeddings 4ã€CLIPEncoder åˆå§‹åŒ– num_hidden_layers ä¸ª CLIPEncoderLayer ä½œä¸º layers éå†ç»è¿‡ layers å¤„ç†ï¼Œå¾—åˆ° layer_outputs è¿”å› hidden_statesã€encoder_statesã€all_attentions hidden_statesï¼šæœ€åä¸€å±‚ layer çš„ hidden_states è¾“å‡º encoder_statesï¼šæ‰€æœ‰ layer çš„ hidden_states è¾“å‡º all_attentionsï¼šæ‰€æœ‰ layer çš„ attn_weights è¾“å‡º 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class CLIPEncoder(nn.Module): def __init__(self, config: CLIPConfig): super().__init__() self.config = config self.layers = nn.ModuleList([CLIPEncoderLayer(config) for _ in range(config.num_hidden_layers)]) self.gradient_checkpointing = False def forward(...): encoder_states = () if output_hidden_states else None all_attentions = () if output_attentions else None hidden_states = inputs_embeds for idx, encoder_layer in enumerate(self.layers): layer_outputs = encoder_layer( hidden_states, attention_mask, causal_attention_mask, output_attentions=output_attentions, ) hidden_states = layer_outputs[0] if output_attentions: all_attentions = all_attentions + (layer_outputs[1],) if output_hidden_states: encoder_states = encoder_states + (hidden_states,) if not return_dict: return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None) return BaseModelOutput( last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions ) 5ã€CLIPEncoderLayer CLIPEncoderLayer ç»“æ„å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå…¶ä¸­ç»è¿‡ self_attn å¤„ç†åå¾—åˆ° hidden_states å’Œ attn_weightsã€‚\nhidden_states ç»åç»­å¤„ç†åå’Œ attn_weights æ‹¼æ¥ï¼Œä½œä¸ºè¾“å‡ºã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class CLIPEncoderLayer(nn.Module): def __init__(self, config: CLIPConfig): super().__init__() self.embed_dim = config.hidden_size self.self_attn = CLIPAttention(config) self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps) self.mlp = CLIPMLP(config) self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps) def forward(): residual = hidden_states hidden_states = self.layer_norm1(hidden_states) hidden_states, attn_weights = self.self_attn( hidden_states=hidden_states, attention_mask=attention_mask, causal_attention_mask=causal_attention_mask, output_attentions=output_attentions, ) hidden_states = residual + hidden_states residual = hidden_states hidden_states = self.layer_norm2(hidden_states) hidden_states = self.mlp(hidden_states) hidden_states = residual + hidden_states outputs = (hidden_states,) if output_attentions: outputs += (attn_weights,) return outputs 6ã€CLIPAttention hidden_states çš„è¾“å…¥å½¢çŠ¶ä¸º(batch_size, 1 + height * width, embed_dim)\nattn_output çš„è¾“å‡ºå½¢çŠ¶ä¸º(batch_size, 1 + height * width, embed_dim)\nattn_weights_reshaped çš„è¾“å‡ºå½¢çŠ¶ä¸º(batch_size * num_heads, 1 + height * width, src_len)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class CLIPAttention(nn.Module): def __init__(self, config): super().__init__() self.config = config self.embed_dim = config.hidden_size self.num_heads = config.num_attention_heads self.head_dim = self.embed_dim // self.num_heads if self.head_dim * self.num_heads != self.embed_dim: raise ValueError( f\u0026#34;embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\u0026#34; f\u0026#34; {self.num_heads}).\u0026#34; ) self.scale = self.head_dim**-0.5 self.dropout = config.attention_dropout self.k_proj = nn.Linear(self.embed_dim, self.embed_dim) self.v_proj = nn.Linear(self.embed_dim, self.embed_dim) self.q_proj = nn.Linear(self.embed_dim, self.embed_dim) self.out_proj = nn.Linear(self.embed_dim, self.embed_dim) def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int): return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous() def forward( self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, causal_attention_mask: Optional[torch.Tensor] = None, output_attentions: Optional[bool] = False, ) -\u0026gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]: \u0026#34;\u0026#34;\u0026#34;Input shape: Batch x Time x Channel\u0026#34;\u0026#34;\u0026#34; bsz, tgt_len, embed_dim = hidden_states.size() # get query proj query_states = self.q_proj(hidden_states) * self.scale key_states = self._shape(self.k_proj(hidden_states), -1, bsz) value_states = self._shape(self.v_proj(hidden_states), -1, bsz) proj_shape = (bsz * self.num_heads, -1, self.head_dim) query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape) key_states = key_states.view(*proj_shape) value_states = value_states.view(*proj_shape) src_len = key_states.size(1) attn_weights = torch.bmm(query_states, key_states.transpose(1, 2)) if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len): raise ValueError( f\u0026#34;Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\u0026#34; f\u0026#34; {attn_weights.size()}\u0026#34; ) # apply the causal_attention_mask first if causal_attention_mask is not None: if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len): raise ValueError( f\u0026#34;Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\u0026#34; f\u0026#34; {causal_attention_mask.size()}\u0026#34; ) attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len) if attention_mask is not None: if attention_mask.size() != (bsz, 1, tgt_len, src_len): raise ValueError( f\u0026#34;Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\u0026#34; ) attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len) attn_weights = nn.functional.softmax(attn_weights, dim=-1) if output_attentions: # this operation is a bit akward, but it\u0026#39;s required to # make sure that attn_weights keeps its gradient. # In order to do so, attn_weights have to reshaped # twice and have to be reused in the following attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len) else: attn_weights_reshaped = None attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training) attn_output = torch.bmm(attn_probs, value_states) if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim): raise ValueError( f\u0026#34;`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\u0026#34; f\u0026#34; {attn_output.size()}\u0026#34; ) attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim) attn_output = attn_output.transpose(1, 2) attn_output = attn_output.reshape(bsz, tgt_len, embed_dim) attn_output = self.out_proj(attn_output) return attn_output, attn_weights_reshaped ","date":"2025-05-20T10:36:50+08:00","permalink":"https://Nyotenn.github.io/Notes/p/clip/","title":"CLIP"},{"content":"Moco ä¸€ã€Introduction We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder.\n1ã€Present situation è¿‘äº›å¹´ï¼Œæ— ç›‘ç£å­¦ä¹ åœ¨ nlp é¢†åŸŸè·å¾—äº†å·¨å¤§çš„æˆæœï¼Œä½†åœ¨ cv é¢†åŸŸä»å’Œç›‘ç£å­¦ä¹ å­˜åœ¨è¾ƒå¤§å·®è·ã€‚\nä½œè€…è®¤ä¸ºï¼Œè¿™ç§å·®è·åœ¨äºå®ƒä»¬å±äºä¸åŒçš„ä¿¡å·ç©ºé—´ã€‚\nè¯­è¨€ä»»åŠ¡æ˜¯åœ¨ç¦»æ•£çš„ä¿¡å·ç©ºé—´ï¼Œå¯ä»¥ç®€å•åœ°æ„é€ å‡ºå­—å…¸ï¼Œè¿™æ˜¯ç”¨äºæ— ç›‘ç£å­¦ä¹ çš„åŸºç¡€ã€‚è€Œå›¾ç‰‡ä»»åŠ¡æ˜¯åœ¨è¿ç»­çš„ã€é«˜ç»´çš„ç©ºé—´ä¸­ï¼Œå®ƒæ›´å…³æ³¨å¦‚ä½•æ„å»ºå­—å…¸ã€‚\n2ã€Related works å¯¹æ¯”å­¦ä¹ \nä¸€éƒ¨åˆ†å¾ˆæœ‰å¸Œæœ›çš„ç ”ç©¶æ˜¯åŸºäºå¯¹æ¯”å­¦ä¹ çš„ã€‚è¿™äº›ç ”ç©¶å¤§ä½“å¯ä»¥æ€»ç»“ä¸ºä»¥ä¸‹æ–¹æ³•ï¼š\nå»ºç«‹ä¸€ä¸ªå­—å…¸ å­—å…¸çš„ key æ˜¯ä»æ ·æœ¬ä¸­é‡‡æ ·ï¼Œç„¶åç»è¿‡ encoder ç¼–ç è€Œæ¥çš„ è®­ç»ƒ encoder ä½¿å¾— query ç¼–ç å’Œå®ƒæ‰€åŒ¹é…çš„æ ·æœ¬ key æ¥è¿‘ï¼Œè€Œå’Œå…¶ä»– key è¿œç¦» 3ã€Moco å’Œä»¥å¾€çš„å¯¹æ¯”å­¦ä¹ ä¸åŒçš„æ˜¯ï¼ŒMocoç”¨ä¸€ä¸ªé˜Ÿåˆ—ä½œä¸ºå­˜å‚¨ key çš„å­—å…¸ï¼Œå½“é˜Ÿåˆ—æ»¡åï¼Œæ–°çš„ mini-batch è¿›æ¥ï¼Œæ—§çš„ mini-batch å‡ºå»ã€‚\né™¤æ­¤ä¹‹å¤–ï¼Œç”±äºå­—å…¸è§„æ¨¡å¤§ï¼Œencoderè¦å­¦ä¹ çš„å‚æ•°å¤šï¼Œå¯¼è‡´ä¸èƒ½é‡‡ç”¨å¸¸è§„çš„ BP æ–¹å¼è¿›è¡Œå­¦ä¹ ï¼Œmoco é‡‡ç”¨äº†åŠ¨é‡ä¸‹é™çš„æ–¹å¼è¿›è¡Œå­¦ä¹ ï¼Œè¿™ä¸€éƒ¨åˆ†åŠ¨é‡æ¥è‡ªäº query encoderã€‚\n1ã€training ä¸Šå›¾åˆ—ä¸¾äº†ä¸‰ç§å¯¹æ¯”æŸå¤±æœºåˆ¶ï¼š\nç¬¬ä¸€ç§ä½¿ç”¨ BP ç®—æ³•è¿›è¡Œç«¯åˆ°ç«¯çš„æ›´æ–° ç¬¬äºŒç§æ˜¯ä» memory bank ä¸­é‡‡æ · key moco é‡‡ç”¨åŠ¨é‡æ›´æ–°çš„ encoder æ¥äº§ç”Ÿ keyï¼Œå¹¶ç»´æŠ¤ä¸€ä¸ªé˜Ÿåˆ—æ¥å­˜å‚¨ key äºŒã€Coding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # f_q, f_k: encoder networks for query and key # queue: dictionary as a queue of K keys (CxK) # m: momentum # t: temperature f_k.params = f_q.params # initialize for x in loader: # load a minibatch x with N samples x_q = aug(x) # a randomly augmented version x_k = aug(x) # another randomly augmented version q = f_q.forward(x_q) # queries: NxC k = f_k.forward(x_k) # keys: NxC k = k.detach() # no gradient to keys # positive logits: Nx1 l_pos = bmm(q.view(N,1,C), k.view(N,C,1)) # negative logits: NxK l_neg = mm(q.view(N,C), queue.view(C,K)) # logits: Nx(1+K) logits = cat([l_pos, l_neg], dim=1) # contrastive loss, Eqn.(1) labels = zeros(N) # positives are the 0-th loss = CrossEntropyLoss(logits/t, labels) # SGD update: query network loss.backward() update(f_q.params) # momentum update: key network f_k.params = m*f_k.params+(1-m)*f_q.params # update dictionary enqueue(queue, k) # enqueue the current minibatch dequeue(queue) # dequeue the earliest minibatch ä¸‰ã€Problems 1ã€Batch Normalization æ­£å¦‚æ ‡å‡†çš„ resnet ä¸€æ ·ï¼Œ$f_q$ å’Œ $f_k$ éƒ½å­˜åœ¨ BN æ“ä½œï¼Œè€Œ BN ä¼šè®¡ç®—æ ·æœ¬çš„æ–¹å·®å’Œå‡å€¼ï¼Œè¿™äº›è¢«ç§°ä¸ºæ³„éœ²ä¿¡æ¯ã€‚\né€šè¿‡è¿™äº›æ³„éœ²çš„ä¿¡æ¯ï¼Œç¼–ç å™¨èƒ½å¤Ÿå¾ˆå®¹æ˜“æ‰¾åˆ°æ­£æ ·æœ¬ï¼Œè€Œéå»å­¦ä¹ ä¸€ä¸ªå¥½çš„æ¨¡å‹ã€‚\nBNæ“ä½œ\nè®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼šå¯¹äºæ¯ä¸€ä¸ªå°æ‰¹é‡ï¼ˆbatchï¼‰çš„æ•°æ®ï¼Œè®¡ç®—å…¶ç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®ã€‚ $$ Î¼ = \\frac{1}{m} \\sum_{i=1}^{m} x_i, \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu)^2 $$ å½’ä¸€åŒ–ï¼šä½¿ç”¨è®¡ç®—å‡ºçš„å‡å€¼å’Œæ–¹å·®æ¥å½’ä¸€åŒ–æ¯ä¸ªæ•°æ®ç‚¹ã€‚ $$ \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} $$ å…¶ä¸­ï¼Œ$\\epsilon$ æ˜¯ä¸€ä¸ªéå¸¸å°çš„æ•°ï¼ˆä¾‹å¦‚ $1e-7$ï¼‰ï¼Œç”¨äºé˜²æ­¢åˆ†æ¯ä¸ºé›¶ã€‚\nç¼©æ”¾å’Œå¹³ç§»ï¼šå½’ä¸€åŒ–åçš„æ•°æ®ä¼šè¢«ç¼©æ”¾ï¼ˆscaleï¼‰å’Œå¹³ç§»ï¼ˆshiftï¼‰ã€‚ $$ y_i = \\gamma \\hat{x}_i + \\beta $$ å…¶ä¸­ï¼Œ$\\gamma$ å’Œ $\\beta$ æ˜¯å¯å­¦ä¹ çš„å‚æ•°ã€‚\nåŸç†\nç”±äºæœ€åä¸€æ­¥åŠ ä¸Šäº†ç¼©æ”¾å’Œå¹³ç§»ï¼Œå¹¶äº¤ç”±ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ï¼Œç¥ç»ç½‘ç»œå› æ­¤å¯æ ¹æ® BN æ“ä½œçš„æ•ˆç›Šæ¥å†³å®šç¼©æ”¾å¹³ç§»å€¼çš„å¤§å°ã€‚\nå¦‚æœå½’ä¸€åŒ–æ“ä½œæ•ˆæœä¸å¥½ï¼Œåˆ™å¯ä»¥é€šè¿‡æ›´æ”¹ç¼©æ”¾å¹³ç§»å€¼æ¥æŠµæ¶ˆéƒ¨åˆ†å½’ä¸€åŒ–çš„æ“ä½œã€‚\n2ã€Why dictionary a larger dictionary may better sample the underlying continuous, high dimensional visual space, while the keys in the dictionary should be represented by the same or similar encoder so that their comparisons to the query are consistent\nä½¿ç”¨ä¸€ä¸ªé˜Ÿåˆ—å¯ä»¥è®©å­—å…¸çš„è§„æ¨¡å¾ˆå¤§ï¼Œä½†åŒæ—¶ä¹Ÿè®© encoder å˜å¾—éš¾ä»¥è®­ç»ƒã€‚å› ä¸ºæ¢¯åº¦ä¸‹é™ä¼šå°†æ¢¯åº¦å†’æ³¡åˆ°å…¨éƒ¨æ ·æœ¬ã€‚\nä¼ ç»Ÿçš„ä¸€ä¸ªè§£å†³æ–¹æ³•æ˜¯ç›´æ¥å°† query encoder ä½œä¸º key encoderã€‚ä½†å®é™…ä¸Šï¼Œè¿™ç§æ–¹æ³•è¡¨ç°å¹¶ä¸å¥½ï¼Œä½œè€…è®¤ä¸ºï¼Œæ˜¯å› ä¸º encoder å˜åŒ–çš„å¤ªå¿«ï¼Œå¯¼è‡´å­—å…¸ä¸­çš„ key ä¸§å¤±äº†ä¸€è‡´æ€§ã€‚\nå› æ­¤ï¼Œmoco æå‡ºäº†åŠ¨é‡æ›´æ–°çš„æ–¹å¼ï¼š $$ \\theta_k \\leftarrow m\\theta_k + (1-m)\\theta_q $$ å…¶ä¸­ï¼Œ$m \\in [0,1)$ æ˜¯åŠ¨é‡ç³»æ•°ï¼Œåªæœ‰ $\\theta_q$ æ˜¯éœ€è¦è¿›è¡Œ BP çš„ã€‚\nå°½ç®¡é˜Ÿåˆ—ä¸­çš„ key å¯èƒ½æ˜¯ä¸åŒç¼–ç å™¨äº§ç”Ÿçš„ï¼Œè¿™äº›ç¼–ç å™¨çš„å·®å¼‚ä¹Ÿå¾ˆå°ï¼ˆå¦‚å– m = 0.999ï¼‰\n","date":"2025-05-20T10:36:50+08:00","permalink":"https://Nyotenn.github.io/Notes/p/moco/","title":"Moco"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;â€”\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\nâ€” Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements â€” abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"https://Nyotenn.github.io/Notes/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu_e95a4276bf860a84.jpg","permalink":"https://Nyotenn.github.io/Notes/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09T00:00:00Z","image":"https://Nyotenn.github.io/Notes/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash_hu_c1ca39d792aee4ab.jpg","permalink":"https://Nyotenn.github.io/Notes/p/placeholder-text/","title":"Placeholder Text"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887â€¦$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","date":"2019-03-08T00:00:00Z","permalink":"https://Nyotenn.github.io/Notes/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\nğŸ™ˆ :see_no_evil: ğŸ™‰ :hear_no_evil: ğŸ™Š :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3 .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","date":"2019-03-05T00:00:00Z","image":"https://Nyotenn.github.io/Notes/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_hu_27b8954607cdb515.jpg","permalink":"https://Nyotenn.github.io/Notes/p/emoji-support/","title":"Emoji Support"}]