[{"content":"GOT 在OCR 1.0时代，典型的OCR系统主要基于多流水线阶段设计。通常包括以下部分：\n元素检测 区域裁剪 字符识别 每个模块容易陷入局部最优，使整个系统的维护成本很高。\n除此之外，传统的OCR方法泛化能力不足，体现为不同的 OCR-1.0网络通常针对不同的子任务设计。\n近些年来，LVLMs 研究迅速发展，展现了令人印象深刻的能力。人们普遍认为光学字符感知和识别是文本驱动图像理解的基础，因此 LVLM 的光学字符识别技术受到了越来越多研究者的关注。\n然而，目前流行的 LVLM 设计可能并不适用于不同的 OCR 任务，原因是：LVLM 主要关注 vision reasoning 性能（例如：VQA），这是 LVLM 所擅长的。为了从 LLM 中快速获得 QA 增益，大多数 LVLM 将图像标记与文本标记对齐。然而，对于纯感知的 OCR 任务，尤其是高密度的文本场景，这样做是不合理的，因为每个对齐的视觉标记(偏向于文本标记)不能压缩足够的字符。\n一、模型和训练 模型\nGOT的模型很简单，包括一个 image encoder、linear layer、output decoder。其中，线性层作为 connector，用于将视觉编码器的通道维度映射到语言解码器中。\n作者采用 ViTDet 作为 encoder，并按照 Vary-tiny 设计 encoder 的最后两层，将 1024 × 1024 × 3 的输入图像转移到 256 × 1024 的图像token。\n然后，通过一个 1024 × 768 的线性层将这些图像标记投影到语言模型中。\n训练\nGOT的训练分三个阶段：\n纯文本识别任务，对视觉编码器进行预训练。使用一个小的解码器（OPT-125M）。在这一阶段，将包含场景文本的图像和包含文档级字符的手动图像输入到模型中，使编码器能够收集两个最常用字符的编码能力； 将预训练好的视觉编码器连接到一个更大的解码器（Qwen-0.5B）。在这一阶段，使用了大量更通用的光学字符识别数据（例如乐谱、数学/分子公式和 Unicode几何图形列表）来扩展这个阶段的光学字符识别知识。 进一步提高其泛化性。生成并添加了细粒度和多页面的合成数据，以支持区域提示OCR、大图像OCR和批处理PDF OCR特性。 二、代码 1、train_GOT.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 os.environ[\u0026#39;NCCL_DEBUG\u0026#39;] = \u0026#39;INFO\u0026#39; os.environ[\u0026#39;OSS_ENDPOINT\u0026#39;] = \u0026#34;http://oss.i.shaipower.com\u0026#34; def train(): # 将类中的属性转换为解析参数 parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments)) model_args, data_args, training_args = parser.parse_args_into_dataclasses() # 加载tokenizer 和 GOTQwenForCausalLM tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name_or_path, trust_remote_code=True, padding_side=\u0026#34;right\u0026#34;, model_max_length=training_args.model_max_length,) model = GOTQwenForCausalLM.from_pretrained(model_args.model_name_or_path, use_safetensors=True) # Resize tokenizer and embedding smart_tokenizer_and_embedding_resize( special_tokens_dict=dict(pad_token=\u0026#39;\u0026lt;|endoftext|\u0026gt;\u0026#39;), tokenizer=tokenizer, model=model) vision_tower_dict = model.get_model().initialize_vision_modules(...) # 初始化 tokenizer model.initialize_vision_tokenizer(...) model.to(dtype=dtype, device=training_args.device) # \u0026#39;image_processor_high # data_args.image_token_len = vision_tower_dict[\u0026#39;image_token_len\u0026#39;] data_args.image_token_len = 256 data_args.image_processor = vision_tower_dict[\u0026#39;image_processor\u0026#39;] data_args.image_processor_high = vision_tower_dict[\u0026#39;image_processor_high\u0026#39;] data_args.use_im_start_end = model_args.use_im_start_end # 冻结 llm，解冻 encoder if model_args.freeze_lm_model: model.requires_grad_(False) for p in model.get_model().mm_projector.parameters(): p.requires_grad = True for p in model.get_model().mm_projector_vary.parameters(): p.requires_grad = True for p in model.get_input_embeddings().parameters(): p.requires_grad = True data_module = make_supervised_data_module( interleave=training_args.interleave, with_box=training_args.with_box, tokenizer=tokenizer, data_args=data_args ) trainer = GOTTrainer( model=model, tokenizer=tokenizer, args=training_args, **data_module) if list(pathlib.Path(training_args.output_dir).glob(\u0026#34;checkpoint-*\u0026#34;)): trainer.train(resume_from_checkpoint=True) else: trainer.train() trainer.save_state() trainer._safe_save(output_dir=training_args.output_dir) if __name__ == \u0026#34;__main__\u0026#34;: train() 2、GOT_ocr_2_0.py GOTQwenModel 图像预处理 Encoder 主体采用 ViTDet架构，Encoder 后采用 Vary 的双卷积设计方案。整个 Encoder 将 $1024\\times1024\\times3$ 的图像压缩为 $256\\times1024$ 的 image tokens 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class GOTQwenModel(Qwen2Model): config_class = GOTConfig def __init__(self, config: Qwen2Config): super(GOTQwenModel, self).__init__(config) self.vision_tower_high = build_vary_vit_b() self.mm_projector_vary = nn.Linear(1024, 1024) def initialize_vision_modules(...): # 图像预处理 image_processor = BlipImageEvalProcessor(image_size=1024) # 1024*1024 image_processor_high = BlipImageEvalProcessor(image_size=1024) # 将图像压缩为 256*1024 的 tokens self.vision_tower_high = self.vision_tower_high.to(dtype=dtype, device=device) # Linear(1024, 1024) 线性映射 self.mm_projector_vary = self.mm_projector_vary.to(dtype=dtype, device=device) image_token_len = 256 return dict( image_processor=image_processor, image_processor_high=image_processor_high, image_token_len=image_token_len, ) def forward(...) -\u0026gt; Union[Tuple, BaseModelOutputWithPast]: vision_tower_high = getattr(self, \u0026#39;vision_tower_high\u0026#39;, None) if vision_tower_high is not None and (input_ids.shape[1] != 1 or self.training) and images is not None: im_patch_token = 151859 im_start_token = 151857 im_end_token = 151858 image_features = [] for image in images: P, C, H, W = image[1].shape if P == 1: with torch.set_grad_enabled(False): cnn_feature = vision_tower_high(image[1]) cnn_feature = cnn_feature.flatten(2).permute(0, 2, 1) # 256*1024 image_feature = self.mm_projector_vary(cnn_feature) image_features.append(image_feature) else: image_patches = torch.unbind(image[1]) image_patches_features = [] for image_patch in image_patches: image_p = torch.stack([image_patch]) with torch.set_grad_enabled(False): cnn_feature_p = vision_tower_high(image_p) cnn_feature_p = cnn_feature_p.flatten(2).permute(0, 2, 1) image_feature_p = self.mm_projector_vary(cnn_feature_p) image_patches_features.append(image_feature_p) image_feature = torch.cat(image_patches_features, dim=1) image_features.append(image_feature) dummy_image_features_2 = torch.zeros(256, 1024, device=inputs_embeds.device, dtype=inputs_embeds.dtype) dummy_image_features = dummy_image_features_2 use_im_start_end = True new_input_embeds = [] for cur_input_ids, cur_input_embeds, cur_image_features in zip(input_ids, inputs_embeds, image_features): # 不包含 image patch 标记，说明是文本，添加零张量并处理下一个样本 if (cur_input_ids == im_patch_token).sum() == 0: # multimodal LLM, but the current sample is not multimodal cur_input_embeds = cur_input_embeds + (0. * dummy_image_features).sum() new_input_embeds.append(cur_input_embeds) continue if use_im_start_end: # 若 start_token 和 end_token 数量匹配，则将图像 token 插入到 embeddings 中 if (cur_input_ids == im_start_token).sum() != (cur_input_ids == im_end_token).sum(): raise ValueError(\u0026#34;The number of image start tokens and image end tokens should be the same.\u0026#34;) image_start_tokens = torch.where(cur_input_ids == im_start_token)[0] for image_start_token_pos, per_cur_image_features in zip(image_start_tokens, cur_image_features): per_cur_image_features = per_cur_image_features.to(device=cur_input_embeds.device) num_patches = per_cur_image_features.shape[0] if cur_input_ids[image_start_token_pos + num_patches + 1] != im_end_token: raise ValueError(\u0026#34;The image end token should follow the image start token.\u0026#34;) cur_input_embeds = torch.cat( ( cur_input_embeds[:image_start_token_pos+1], per_cur_image_features, cur_input_embeds[image_start_token_pos + num_patches + 1:] ), dim=0 ) new_input_embeds.append(cur_input_embeds) else: raise NotImplementedError inputs_embeds = torch.stack(new_input_embeds, dim=0) return super(GOTQwenModel, self).forward( input_ids=None, attention_mask=attention_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, position_ids = position_ids, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict ) GOTQwenForCausalLM 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 class GOTQwenForCausalLM(Qwen2ForCausalLM): config_class = GOTConfig def __init__(self, config): super(Qwen2ForCausalLM, self).__init__(config) self.model = GOTQwenModel(config) self.vocab_size = config.vocab_size self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False) # Initialize weights and apply final processing self.post_init() def get_model(self): return self.model def forward(...) -\u0026gt; Union[Tuple, CausalLMOutputWithPast]: outputs = self.model(...) hidden_states = outputs[0] logits = self.lm_head(hidden_states) logits = logits.float() # logits loss = None if labels is not None: # Shift so that tokens \u0026lt; n predict n shift_logits = logits[..., :-1, :].contiguous() shift_labels = labels[..., 1:].contiguous() # Flatten the tokens loss_fct = CrossEntropyLoss() shift_logits = shift_logits.view(-1, self.config.vocab_size) shift_labels = shift_labels.view(-1) # Enable model parallelism shift_labels = shift_labels.to(shift_logits.device) loss = loss_fct(shift_logits, shift_labels) if not return_dict: output = (logits,) + outputs[1:] return (loss,) + output if loss is not None else output return CausalLMOutputWithPast( loss=loss, logits=logits, past_key_values=outputs.past_key_values, hidden_states=outputs.hidden_states, attentions=outputs.attentions, ) def prepare_inputs_for_generation( self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs ): # Omit tokens covered by past_key_values if past_key_values is not None: if isinstance(past_key_values, Cache): cache_length = past_key_values.get_seq_length() past_length = past_key_values.seen_tokens max_cache_length = past_key_values.get_max_length() else: cache_length = past_length = past_key_values[0][0].shape[2] max_cache_length = None # Keep only the unprocessed tokens: # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as # input) if attention_mask is not None and attention_mask.shape[1] \u0026gt; input_ids.shape[1]: input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :] # 2 - If the past_length is smaller than input_ids\u0026#39;, then input_ids holds all input tokens. We can discard # input_ids based on the past_length. elif past_length \u0026lt; input_ids.shape[1]: input_ids = input_ids[:, past_length:] # 3 - Otherwise (past_length \u0026gt;= input_ids.shape[1]), let\u0026#39;s assume input_ids only has unprocessed tokens. # If we are about to go beyond the maximum cache length, we need to crop the input attention mask. if ( max_cache_length is not None and attention_mask is not None and cache_length + input_ids.shape[1] \u0026gt; max_cache_length ): attention_mask = attention_mask[:, -max_cache_length:] position_ids = kwargs.get(\u0026#34;position_ids\u0026#34;, None) if attention_mask is not None and position_ids is None: # create position_ids on the fly for batch generation position_ids = attention_mask.long().cumsum(-1) - 1 position_ids.masked_fill_(attention_mask == 0, 1) if past_key_values: position_ids = position_ids[:, -input_ids.shape[1] :] # if `inputs_embeds` are passed, we only want to use them in the 1st generation step if inputs_embeds is not None and past_key_values is None: model_inputs = {\u0026#34;inputs_embeds\u0026#34;: inputs_embeds} else: model_inputs = {\u0026#34;input_ids\u0026#34;: input_ids} model_inputs.update( { \u0026#34;position_ids\u0026#34;: position_ids, \u0026#34;past_key_values\u0026#34;: past_key_values, \u0026#34;use_cache\u0026#34;: kwargs.get(\u0026#34;use_cache\u0026#34;), \u0026#34;attention_mask\u0026#34;: attention_mask, \u0026#34;images\u0026#34;: kwargs.get(\u0026#34;images\u0026#34;, None), } ) return model_inputs def initialize_vision_tokenizer(...): self.resize_token_embeddings(len(tokenizer)) if config.use_im_start_end: self.resize_token_embeddings(len(tokenizer)) config.im_start_token, config.im_end_token = 151857, 151858 3、vary_b.py _build_vary path size 为 16*16，image size 为 1024*1024\n以此尺寸为参数初始化 ViT encoder\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def build_vary_vit_b(checkpoint=None): return _build_vary(...) def _build_vary( encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, checkpoint=None, ): prompt_embed_dim = 256 image_size = 1024 vit_patch_size = 16 image_embedding_size = image_size // vit_patch_size image_encoder=ImageEncoderViT(...) return image_encoder ImageEncoderViT 将图片转为 patch embedding 将绝对位置编码加到 patch embedding 中（可选） 经过 depth 个 Block 和2个卷积层处理，将图像压缩为 256*1024 的 tokens 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class ImageEncoderViT(nn.Module): def __init__(...) -\u0026gt; None: super().__init__() self.img_size = img_size # 过一个卷积层，把图片转为patch embedding self.patch_embed = PatchEmbed(...) # 初始化绝对位置编码 self.pos_embed: Optional[nn.Parameter] = None if use_abs_pos: self.pos_embed = nn.Parameter( torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim) ) self.blocks = nn.ModuleList() for i in range(depth): block = Block(...) self.blocks.append(block) self.neck = nn.Sequential( nn.Conv2d( embed_dim, out_chans, kernel_size=1, bias=False, ), LayerNorm2d(out_chans), nn.Conv2d( out_chans, out_chans, kernel_size=3, padding=1, bias=False, ), LayerNorm2d(out_chans), ) self.net_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False) self.net_3 = nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1, bias=False) def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: x = self.patch_embed(x) if self.pos_embed is not None: x = x + self.pos_embed for blk in self.blocks: x = blk(x) x = self.neck(x.permute(0, 3, 1, 2)) x = self.net_2(x) x = self.net_3(x) return x Block 一个 Block 包含一个 Attention 层和一个 MLPBlock 层。在这两个层之前都经过了 layer norm 处理。\n其中，一个 MLPBlock 层包含两个线性层。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Block(nn.Module): \u0026#34;\u0026#34;\u0026#34;Transformer blocks with support of window attention and residual propagation blocks\u0026#34;\u0026#34;\u0026#34; def __init__(...) -\u0026gt; None: super().__init__() self.norm1 = norm_layer(dim) self.attn = Attention(...) self.norm2 = norm_layer(dim) self.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer) self.window_size = window_size def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: shortcut = x x = self.norm1(x) # Window partition if self.window_size \u0026gt; 0: H, W = x.shape[1], x.shape[2] x, pad_hw = window_partition(x, self.window_size) x = self.attn(x) # Reverse window partition if self.window_size \u0026gt; 0: x = window_unpartition(x, self.window_size, pad_hw, (H, W)) x = shortcut + x x = x + self.mlp(self.norm2(x)) return x 4、/data/__init__.py DataCollatorForSupervisedDataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @dataclass class DataCollatorForSupervisedDataset(object): tokenizer: transformers.PreTrainedTokenizer def __call__(self, instances): input_ids, labels = tuple([instance[key] for instance in instances] for key in (\u0026#34;input_ids\u0026#34;, \u0026#34;labels\u0026#34;)) images = [torch.stack(instance[\u0026#39;image\u0026#39;]) for instance in instances] images_high = [torch.stack(instance[\u0026#39;image_high\u0026#39;]) for instance in instances] images = list(zip(images, images_high)) input_ids = torch.nn.utils.rnn.pad_sequence( input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id) labels = torch.nn.utils.rnn.pad_sequence( labels, batch_first=True, padding_value=IGNORE_INDEX) batch = dict( input_ids=input_ids, labels=labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id), images=images, ) return batch def make_supervised_data_module(interleave, with_box, tokenizer, data_args): if data_args.conversation_version == \u0026#39;mpt\u0026#39;: from GOT.data.conversation_dataset_qwen import ConversationDataset dataset_cls = ConversationDataset train_dataset = dataset_cls( tokenizer=tokenizer, datasets=data_args.datasets, multimodal_cfg=dict( sep_image_conv_front=data_args.sep_image_conv_front, image_token_len=data_args.image_token_len, image_aspect_ratio=data_args.image_aspect_ratio, use_im_start_end=data_args.use_im_start_end, image_processor=data_args.image_processor, image_processor_high = data_args.image_processor_high, box_limit=data_args.box_limit, ) ) data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer) return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator) 5、trainer_vit_fixlr.py GOTTrainer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class GOTTrainer(Trainer): def _safe_save(self, output_dir: str): \u0026#34;\u0026#34;\u0026#34;Collects the state dict and dump to disk.\u0026#34;\u0026#34;\u0026#34; state_dict = self.model.state_dict() if self.args.should_save: cpu_state_dict = { key: value.cpu() for key, value in state_dict.items() } del state_dict self._save(output_dir, state_dict=cpu_state_dict) # noqa def _save(self, output_dir: Optional[str] = None, state_dict=None): if getattr(self.args, \u0026#39;tune_mm_mlp_adapter\u0026#39;, False): # Save the model _state_dict = state_dict if _state_dict is None: # Only save the model itself if we are using distributed training model_to_save = unwrap_model(self.model) _state_dict = model_to_save.state_dict() weight_to_save = {} keys_to_match = [\u0026#39;mm_projector\u0026#39;, \u0026#39;embed_tokens\u0026#39;, \u0026#39;embed_in\u0026#39;] for k, v in _state_dict.items(): if any(key_match in k for key_match in keys_to_match): weight_to_save[k] = v current_folder = output_dir.split(\u0026#39;/\u0026#39;)[-1] parent_folder = os.path.dirname(output_dir) if current_folder.startswith(\u0026#39;checkpoint-\u0026#39;): mm_projector_folder = os.path.join(parent_folder, \u0026#34;mm_projector\u0026#34;) os.makedirs(mm_projector_folder, exist_ok=True) torch.save(weight_to_save, os.path.join(mm_projector_folder, f\u0026#39;{current_folder}.bin\u0026#39;)) else: torch.save(weight_to_save, os.path.join(output_dir, f\u0026#39;mm_projector.bin\u0026#39;)) super(GOTTrainer, self)._save(output_dir, state_dict) def create_optimizer(self): \u0026#34;\u0026#34;\u0026#34; Setup the optimizer. We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the Trainer\u0026#39;s init through `optimizers`, or subclass and override this method in a subclass. \u0026#34;\u0026#34;\u0026#34; opt_model = self.model if self.optimizer is None: decay_parameters = get_parameter_names(opt_model, ALL_LAYERNORM_LAYERS) decay_parameters = [name for name in decay_parameters if \u0026#34;bias\u0026#34; not in name] optimizer_grouped_parameters = [ { \u0026#34;params\u0026#34;: [ p for n, p in opt_model.named_parameters() if \u0026#39;vision_encoder\u0026#39; in n and n in decay_parameters and p.requires_grad ], \u0026#34;weight_decay\u0026#34;: self.args.weight_decay, \u0026#34;lr\u0026#34;: self.args.learning_rate, }, { \u0026#34;params\u0026#34;: [ p for n, p in opt_model.named_parameters() if \u0026#39;vision_encoder\u0026#39; in n and n not in decay_parameters and p.requires_grad], \u0026#34;weight_decay\u0026#34;: 0.0, \u0026#34;lr\u0026#34;: self.args.learning_rate, }, { \u0026#34;params\u0026#34;: [ p for n, p in opt_model.named_parameters() if \u0026#39;vision_encoder\u0026#39; not in n and n in decay_parameters and p.requires_grad], \u0026#34;weight_decay\u0026#34;: self.args.weight_decay, \u0026#34;lr\u0026#34;: self.args.learning_rate, }, { \u0026#34;params\u0026#34;: [ p for n, p in opt_model.named_parameters() if \u0026#39;vision_encoder\u0026#39; not in n and n not in decay_parameters and p.requires_grad ], \u0026#34;weight_decay\u0026#34;: 0.0, \u0026#34;lr\u0026#34;: self.args.learning_rate, }, ] for idx, group in enumerate(optimizer_grouped_parameters): print(idx, len(group[\u0026#39;params\u0026#39;]), group[\u0026#39;lr\u0026#39;]) optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args) self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs) return self.optimizer 6、run_ocr_2.0.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 DEFAULT_IMAGE_TOKEN = \u0026#34;\u0026lt;image\u0026gt;\u0026#34; DEFAULT_IMAGE_PATCH_TOKEN = \u0026#39;\u0026lt;imgpad\u0026gt;\u0026#39; DEFAULT_IM_START_TOKEN = \u0026#39;\u0026lt;img\u0026gt;\u0026#39; DEFAULT_IM_END_TOKEN = \u0026#39;\u0026lt;/img\u0026gt;\u0026#39; # 将中文逗号和句号转为英文的 translation_table = str.maketrans(punctuation_dict) def load_image(image_file): if image_file.startswith(\u0026#39;http\u0026#39;) or image_file.startswith(\u0026#39;https\u0026#39;): response = requests.get(image_file) image = Image.open(BytesIO(response.content)).convert(\u0026#39;RGB\u0026#39;) else: image = Image.open(image_file).convert(\u0026#39;RGB\u0026#39;) return image def eval_model(args): tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) model = GOTQwenForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, device_map=\u0026#39;cuda\u0026#39;, use_safetensors=True, pad_token_id=151643).eval() model.to(device=\u0026#39;cuda\u0026#39;, dtype=torch.bfloat16) # TODO vary old codes, NEED del image_processor = BlipImageEvalProcessor(image_size=1024) image_processor_high = BlipImageEvalProcessor(image_size=1024) use_im_start_end = True image_token_len = 256 image = load_image(args.image_file) w, h = image.size if args.type == \u0026#39;format\u0026#39;: qs = \u0026#39;OCR with format: \u0026#39; else: qs = \u0026#39;OCR: \u0026#39; if args.box: bbox = eval(args.box) if len(bbox) == 2: bbox[0] = int(bbox[0]/w*1000) bbox[1] = int(bbox[1]/h*1000) if len(bbox) == 4: bbox[0] = int(bbox[0]/w*1000) bbox[1] = int(bbox[1]/h*1000) bbox[2] = int(bbox[2]/w*1000) bbox[3] = int(bbox[3]/h*1000) if args.type == \u0026#39;format\u0026#39;: qs = str(bbox) + \u0026#39; \u0026#39; + \u0026#39;OCR with format: \u0026#39; else: qs = str(bbox) + \u0026#39; \u0026#39; + \u0026#39;OCR: \u0026#39; if args.color: if args.type == \u0026#39;format\u0026#39;: qs = \u0026#39;[\u0026#39; + args.color + \u0026#39;]\u0026#39; + \u0026#39; \u0026#39; + \u0026#39;OCR with format: \u0026#39; else: qs = \u0026#39;[\u0026#39; + args.color + \u0026#39;]\u0026#39; + \u0026#39; \u0026#39; + \u0026#39;OCR: \u0026#39; if use_im_start_end: qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN*image_token_len + DEFAULT_IM_END_TOKEN + \u0026#39;\\n\u0026#39; + qs else: qs = DEFAULT_IMAGE_TOKEN + \u0026#39;\\n\u0026#39; + qs conv_mode = \u0026#34;mpt\u0026#34; args.conv_mode = conv_mode conv = conv_templates[args.conv_mode].copy() conv.append_message(conv.roles[0], qs) conv.append_message(conv.roles[1], None) prompt = conv.get_prompt() inputs = tokenizer([prompt]) # vary old codes, no use image_1 = image.copy() image_tensor = image_processor(image) image_tensor_1 = image_processor_high(image_1) input_ids = torch.as_tensor(inputs.input_ids).cuda() stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2 keywords = [stop_str] stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids) streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) with torch.autocast(\u0026#34;cuda\u0026#34;, dtype=torch.bfloat16): output_ids = model.generate( input_ids, images=[(image_tensor.unsqueeze(0).half().cuda(), image_tensor_1.unsqueeze(0).half().cuda())], do_sample=False, num_beams = 1, no_repeat_ngram_size = 20, streamer=streamer, max_new_tokens=4096, stopping_criteria=[stopping_criteria] ) if args.render: print(\u0026#39;==============rendering===============\u0026#39;) outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip() if outputs.endswith(stop_str): outputs = outputs[:-len(stop_str)] outputs = outputs.strip() if \u0026#39;**kern\u0026#39; in outputs: import verovio from cairosvg import svg2png import cv2 import numpy as np tk = verovio.toolkit() tk.loadData(outputs) tk.setOptions({\u0026#34;pageWidth\u0026#34;: 2100, \u0026#34;footer\u0026#34;: \u0026#39;none\u0026#39;, \u0026#39;barLineWidth\u0026#39;: 0.5, \u0026#39;beamMaxSlope\u0026#39;: 15, \u0026#39;staffLineWidth\u0026#39;: 0.2, \u0026#39;spacingStaff\u0026#39;: 6}) tk.getPageCount() svg = tk.renderToSVG() svg = svg.replace(\u0026#34;overflow=\\\u0026#34;inherit\\\u0026#34;\u0026#34;, \u0026#34;overflow=\\\u0026#34;visible\\\u0026#34;\u0026#34;) svg_to_html(svg, \u0026#34;./results/demo.html\u0026#34;) if args.type == \u0026#39;format\u0026#39; and \u0026#39;**kern\u0026#39; not in outputs: if \u0026#39;\\\\begin{tikzpicture}\u0026#39; not in outputs: html_path = \u0026#34;./render_tools/\u0026#34; + \u0026#34;/content-mmd-to-html.html\u0026#34; html_path_2 = \u0026#34;./results/demo.html\u0026#34; right_num = outputs.count(\u0026#39;\\\\right\u0026#39;) left_num = outputs.count(\u0026#39;\\left\u0026#39;) if right_num != left_num: outputs = outputs.replace(\u0026#39;\\left(\u0026#39;, \u0026#39;(\u0026#39;).replace(\u0026#39;\\\\right)\u0026#39;, \u0026#39;)\u0026#39;).replace(\u0026#39;\\left[\u0026#39;, \u0026#39;[\u0026#39;).replace(\u0026#39;\\\\right]\u0026#39;, \u0026#39;]\u0026#39;).replace(\u0026#39;\\left{\u0026#39;, \u0026#39;{\u0026#39;).replace(\u0026#39;\\\\right}\u0026#39;, \u0026#39;}\u0026#39;).replace(\u0026#39;\\left|\u0026#39;, \u0026#39;|\u0026#39;).replace(\u0026#39;\\\\right|\u0026#39;, \u0026#39;|\u0026#39;).replace(\u0026#39;\\left.\u0026#39;, \u0026#39;.\u0026#39;).replace(\u0026#39;\\\\right.\u0026#39;, \u0026#39;.\u0026#39;) outputs = outputs.replace(\u0026#39;\u0026#34;\u0026#39;, \u0026#39;``\u0026#39;).replace(\u0026#39;$\u0026#39;, \u0026#39;\u0026#39;) outputs_list = outputs.split(\u0026#39;\\n\u0026#39;) gt= \u0026#39;\u0026#39; for out in outputs_list: gt += \u0026#39;\u0026#34;\u0026#39; + out.replace(\u0026#39;\\\\\u0026#39;, \u0026#39;\\\\\\\\\u0026#39;) + r\u0026#39;\\n\u0026#39; + \u0026#39;\u0026#34;\u0026#39; + \u0026#39;+\u0026#39; + \u0026#39;\\n\u0026#39; gt = gt[:-2] with open(html_path, \u0026#39;r\u0026#39;) as web_f: lines = web_f.read() lines = lines.split(\u0026#34;const text =\u0026#34;) new_web = lines[0] + \u0026#39;const text =\u0026#39; + gt + lines[1] else: html_path = \u0026#34;./render_tools/\u0026#34; + \u0026#34;/tikz.html\u0026#34; html_path_2 = \u0026#34;./results/demo.html\u0026#34; outputs = outputs.translate(translation_table) outputs_list = outputs.split(\u0026#39;\\n\u0026#39;) gt= \u0026#39;\u0026#39; for out in outputs_list: if out: if \u0026#39;\\\\begin{tikzpicture}\u0026#39; not in out and \u0026#39;\\\\end{tikzpicture}\u0026#39; not in out: while out[-1] == \u0026#39; \u0026#39;: out = out[:-1] if out is None: break if out: if out[-1] != \u0026#39;;\u0026#39;: gt += out[:-1] + \u0026#39;;\\n\u0026#39; else: gt += out + \u0026#39;\\n\u0026#39; else: gt += out + \u0026#39;\\n\u0026#39; with open(html_path, \u0026#39;r\u0026#39;) as web_f: lines = web_f.read() lines = lines.split(\u0026#34;const text =\u0026#34;) new_web = lines[0] + gt + lines[1] with open(html_path_2, \u0026#39;w\u0026#39;) as web_f_new: web_f_new.write(new_web) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#34;--model-name\u0026#34;, type=str, default=\u0026#34;facebook/opt-350m\u0026#34;) parser.add_argument(\u0026#34;--image-file\u0026#34;, type=str, required=True) parser.add_argument(\u0026#34;--type\u0026#34;, type=str, required=True) parser.add_argument(\u0026#34;--box\u0026#34;, type=str, default= \u0026#39;\u0026#39;) parser.add_argument(\u0026#34;--color\u0026#34;, type=str, default= \u0026#39;\u0026#39;) parser.add_argument(\u0026#34;--render\u0026#34;, action=\u0026#39;store_true\u0026#39;) args = parser.parse_args() eval_model(args) ","date":"2025-05-20T16:06:50+08:00","permalink":"https://Nyotenn.github.io/Notes/p/got/","title":"GOT"},{"content":"OCR-free 一、Donut 论文地址：OCR-free Document Understanding Transformer\n作者机构：NAVER CLOVA\n发表时间：2022\n发表情况：ECCV 2022\n代码仓库：https://github.com/clovaai/donu\nOCR-Base OCR-Free 优点 性能高 端到端训练； 对OCR结果校正依赖小 缺点 使用OCR的计算成本高; OCR模型在语言或文档类型上缺乏灵活性; OCR误差传播到后续过程。 性能差； 需要联合优化； Donut的训练采用了预训练和微调方案。在预训练阶段，Donut 通过结合图像和之前的文本上下文来预测下一个单词，从而学习如何阅读文本。在微调阶段，Donut 根据下游任务学习如何理解整个文档。\nDonut 是一个端到端的VDU模型，用于对文档图像进行一般理解。Donut的架构非常简单，它由基于 Transformer 的视觉 Encoder 和文本 Decoder 模块组成。Donut 不依赖于任何与OCR功能相关的模块，而是使用视觉 Encoder 从给定的文档图像中提取特征。下面的文本 Decoder 将派生的特征映射到子词 token 序列中，以构建所需的结构化格式（例如JSON）。\n二、Nougat **原文：**Nougat： Neural Optical Understanding for Academic Documents\n**作者：**Lukas Blecher∗ Guillem Cucurull Thomas Scialom, MetaAI\n代码：https://github.com/facebookresearch/nougat https://github.com/allenai/s2or\nNougat 和 Donut 一样，是标准的 Encoder-Decoder 架构。\nEncoder 用的是 20层 （Swin-B {2, 2, 14, 2}）的 Swin Transformer。\nDecoder 用的是文字生成模型 mBART 中decoder，可以看成比较标准的 transformer decoder。\n该篇工作基于Donut，整个网络结构都是来自Donut。因此该篇工作的主要贡献不在算法侧，而是在于数据侧，主要有以下三点：\n训练了pre-trained model，可以转换PDF到轻量的mardown格式，包括PDF中表格和公式；\n通过构造了一个完整pipeline，来构造一个将PDF转换为对应MMD格式的数据集；\n该方法其输入仅仅是图像，且能够处理扫描的论文和书籍；\n三、Pix2Struct code: https://github.com/google-research/pix2struct\n该篇工作主要是将网页上masked screenshots转换简单的HTML代码，示例图如上面所示，第一行是模型的输入，第二行是模型对应的输出结果。\n模型的结构是基于ViT的图像编码器加文本解码器，对于模型的输入进行了一些改变以适应各种分辨率的图像输入。传统ViT会将输入图像缩放到定义好的分辨率，如下面左图所示，这会带来两个缺点，首先是图像中的文字、ui会发生畸变，这不利于对这些元素的识别。其次就是预训练阶段只观察到一个特定的分辨率，之后迁移到一些需要更高分辨率的下游任务是很困难的。 但是作者这里对ViT输入做了一些改进，首先等比例缩放图像，保证固定尺寸的patch个数能够达到最大序列长度，之后使用二维绝对位置嵌入。这样有两个好处，首先可以应对极端的图像长宽比，其次可以实时修改序列长度和分辨率。\n[!NOTE]\n好处：可以维持肉眼所见的文字尺度，不会由于图像缩放导致文字畸变；\n坏处：不同于文档文本，场景中的文本（如广告牌等）本身存在文字变形。所以训练中使用的图像缩放可能导致后续存在更好的泛化性能。\n四、Vary 该篇工作着眼于dense and fine-grained vision perception任务，例如文档级别OCR和图表理解任务。这一点和Nougat工作有些类似。论文主要可分为两个阶段：\n设计一个vocabulary network，通过自回归的方式产生所需要的vocabulary 将上一阶段的new vocabulary与LLM中现有的vanilla vision vocabulary融合，使得LLM获得新的能力。 因为在第一阶段训练Vary-tiny时，image encoder用的是在ViTDet上训练所得SAM-base模型，所以其输入图像尺寸是1024x1024x3。这一点可以有效缓解文档类图像在较小尺寸下输入，文字模糊变形问题。但是并未从实际上提高模型的输入尺寸。\n五、Unidoc 论文地址：UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding\nUniDoc是一个可以同时做文本检测、文本识别和文档理解的多模态模型。\n如图所示，UniDoc 将文本检测、识别、spotting 以及多模态理解等任务，在多模态指令微调框架中实现了统一。具体来说，输入图像和指令（例如检测、识别、spotting或语义理解）后，UniDoc 会从图像中提取视觉和文本信息，并基于大型语言模型的知识库，结合指令内容完成回答。\n以 CLIP 为编码器，将图像I抽象为视觉特征图 将视觉特征图展平为sequence，并将其投影到LLM的嵌入维度中，输出 $E_v$ 将语言指令Q转为序列 $E_l$ 将 $E_v$ 和 $E_l$ 拼接，并送到 Vicuna 中 Vicuna 根据收到的视觉和文本提示生成响应 训练总共分两个阶段：\n在预训练阶段，冻结视觉和语言模型，只训练 Projector。\n在微调阶段，解冻所有模型。\n六、CogVLM 整体结构在LLaVA的基础上在大语言模型中每层都添加了一个可训练的视觉专家模块（右图），以实现图像特征和文本特征的深度对齐，而不会牺牲任何NLP任务的性能。只有紫色部分是可训练的。\n训练\n训练和LLaVA同样分为两个阶段： 第一阶段：预训练阶段；使用了数据集LAION-2B和COYO-700M，加上4M的视觉定位数据集（LAION-400M的子集）。预训练阶段也分为两个训练任务（文本下一个Token预测和REC任务）。REC任务是指给我图片中目标的标书，预测Bounding Box的任务。在这个阶段冻结了VIT和语言模型原本的参数，可训练的参数为6.5B。\n第二阶段：微调；数据来源于LLaVA-Instruct（GPT4生成，手动纠正了该数据集中的错误）、LRV-Instruction、LLaVAR和内部数据集。在该阶段，除了ViT Encoder，其它参数都是可训练的。\n七、CogAgent 在CogVLM的基础上添加了一个高分辨率的图片编码器模块（High-resolution image feature），解决了CogVLM对高分辨图片支持弱的问题, 可以集成到各种视觉语言模型架构中。\n训练\n预训练阶段\n训练主要分为三个任务：文本识别、视觉定位和GUI图像理解（指网页等GUI图像的理解能力）。\n文本识别任务数据集主要来自合成的文本图像(80M)、自然图像OCR(18M)、学术文档（9M、arXiv）。 视觉定位数据集：LAION-115M中得到的40M数据, 一个图像+对应的描述 比如A bluebird [[302,413,640,752]]sitting on abranch coffee mugs [[279,588,677,804]]。 GUI图像理解：GUI指代表达生成、GUI指代表达理解。 预训练也分为两个阶段，第一阶段只训练新添加的High-Resolution Cross-Module（参数约为646M）；第二阶段训练High-Resolution Cross-Module和视觉专家模块。在预训练阶段会根据任务的难度按照递增的顺序进行训练，可以令模型收敛快并且训练稳定。\n微调阶段\n数据集包含多个公开的视觉问答数据集、Mind2Web、AITW等。\n在这个阶段所有模型均可以参与微调。\n八、mPlug-Owl Arxiv：https://arxiv.org/abs/2304.1417\nmPLUG-Owl的模型结构如上图所示，输入一张图片，经过一个Visual Encoder(对图像编码)和Visual Abstractor（将较长的、细粒度的图像特征概括为少量可学习的 Token，实现对视觉信息的高效建模) ，然后生成的视觉 Token和文本查询送到大语言模型获取输出。\n训练\n第一阶段：使用多模态数据训练视觉模块（冻结语言模块），可以让视觉特征贴合语言特征。 第二阶段：使用多模态和单模态数据联合调整语言模块的LoRA参数，冻结视觉模块。模型可以学习多样化的单模态和多模态指令，同时具备单模态和多模态多轮对话能力。 不同于LLaVA，mPLUG-Owl使用类似Blip中的Q-Former结构进行视觉特征提取，提取到的特征的维度同Query的维度相同（维度太小），会造成信息损失。\n九、mPLUG-DocOwl 已有的多模态大模型在未经领域训练的情况下，往往会忽略细粒度的OCR 特征，例如复杂的表格、或是大的文本块等。\n例如，现有的模型大致分为两种：基于现成的OCR模型、以及end-to-end的模型。然而，两种方法都需要在特定数据集上进行微调，促进图像和文本之间的跨模态对齐，无法达到大语言模型的开放域指令理解的能力（即模型能够理解来自各种不同来源和背景的指令或请求，而不仅仅局限于某一特定领域或数据集）。\nmPLUG-DocOwl是在mPLUG-Owl基础上，基于统一的instruction tuning，来平衡language-only, general vision-and-language和document understanding三个任务。\n训练\nmPLUG-DocOwl收集了具有不同任务格式的各种文档理解数据集，包括视觉问答（VQA），信息提取（IE），自然语言推理（NLI）和图像字幕（IC）。mPLUG-Owl 以统一格式执行指令调优，即“Human：{question} AI：{answer}”。通过替换 {question} 和 {answer} 占位符，将不同的文档理解任务转换为与 mPLUG-Owl 相同的格式。\n**第一阶段：**初始化于mPLUG-Owl，为了增强模型的文字理解能力，mPLUG-DocOwl第一阶段只采用新构建的带文字图片相关的指令数据集进行微调，训练结构包括visual abstractor和LLM中的LoRA。 第二阶段：为了保持Owl的开放域图文理解以及指令理解能力，mPLUG-DocOwl第二阶段进一步添加Owl的指令微调数据进行混合训练，只训练LLM中的LoRA。 十、Monkey 之前的模型，如LLaVA和Qwen-VL，在初始训练中使用了LAION、COYO和CC3M等大型数据集。但这些数据集通常提供过于简单的图像文本对（用一个短句来描述一张复杂的图片），缺乏详细的视觉信息。因此，即使这些模型是用高分辨率的图像进行训练的，它们仍然很难准确地将视觉特征与基本标注联系起来。这一局限性影响了模型有效地结合视觉处理和语言理解的能力。\n为了弥合这一差距，开发了一种新的方法来自动生成多级描述，旨在通过有效地融合来自各种生成器的输出来创建丰富且高质量的Caption数据。 利用了几个先进的VLLM组合： BLIP2提供了对图像和文本之间关系的深入理解； PPOCR是光学字符识别领域表现优异的模型； GRIT 专注于详细的图像 - 文本匹配； SAM 专注于语义对齐； 而 ChatGPT 擅长上下文语言生成。\n如上图所示，图像描述过程从BLIP2使用QFormer生成总体标题开始，以便与视觉编码器和LLM进行集成，同时保留原始CC3M注释以提供上下文。然后，区域到文本模型GRIT生成对特定区域、对象及其特征的详细描述。PPOCR从图像中提取文本，而SAM分割并识别对象及其部分，随后被由BLIP2分别描述，然后进一步使用BLIP2来检查图像区域、对象及其描述之间的连贯性，并过滤掉低分匹配项。最后，所有数据，包括全局标题、本地化描述、以及带有空间坐标的对象详细信息，都输入到ChatGPT API进行微调，使ChatGPT能够生成准确且上下文丰富的图像描述。通过合并这些系统的独特特征，实现了分层和全面的Caption生成风格。 它捕捉了广泛的视觉和文本细微差别，生成了不仅详细，而且在上下文中多样且引人入胜的字幕。\n模型主要是为提高图片的分辨率做了一些改进，相比于CogAgent和Vary通过增加一个高分辨率图编码器来提高图片分辨率，Monkey是将图片分割为几个448*448的 Patch，每个Patch都经过相同的编码器（冻结的VIT编码器，原图resize之后也经过VIT），通过从原始图像中捕获全局特征和分割的区域特征来实现高分辨率，每个图片Patch由静态视觉编码器独立处理，并使用LoRA调整和可训练的视觉重采样器进行增强。好处就是无需预训练的情况下支持高达 1344x896 的分辨率。随后，使用share resampler(作用就是把每个Patch编码的输入整合为一个（汇总视觉信息并在语言特征空间中获得更高语义的视觉表示，使用一个交叉注意模块来实现这一点。该模块使用可训练向量作为Query向量，并使用来自视觉编码器的图像特征作为交叉注意操作的Key，QFormer吧)，然后送入LLM生成所需的答案。\n十一、TextMonkey Monkey的优化版本，因为Monkey把大分辨率的图片分割为几个Patch然后分别单独输入进图像编码器会导致信息的不连贯。主要优化方法是采用 Shfited Window Attention 跨Patch连接。另外进行了token压缩。\n","date":"2025-05-20T16:06:50+08:00","permalink":"https://Nyotenn.github.io/Notes/p/ocr-free/","title":"OCR-free"},{"content":"DETR 一、Introduction 1、DETR的目标 The goal of object detection is to predict a set of bounding boxes and category labels for each object of interest.\nDETR（Detection Transformer）是一种用于目标检测（Object Detection）的模型，该模型在2020年由Facebook AI Research（FAIR）提出。DETR的主要贡献在于，它使用Transformer架构来解决目标检测问题，从而避免了传统目标检测算法中常见的一些复杂性，如锚框（anchor boxes）、非极大值抑制（NMS）等。\n2、主要构成 Backbone Network: 通常使用ResNet或其他卷积神经网络作为基础网络，用于提取图像特征。 Transformer Encoder-Decoder: 在图像特征的基础上，使用Transformer的编码器-解码器结构进行目标检测。 Bipartite Matching Loss: 这是一个新颖的损失函数，用于在训练期间匹配预测框和真实框。 3、优点 简单性: 不需要复杂的锚框和NMS步骤。 端到端训练: 整个模型可以一次性、端到端地进行训练。 可扩展性: 可以容易地扩展到其他视觉任务。 4、缺点 速度: 由于Transformer的复杂性，模型的推理速度可能不如一些更简单的模型。 训练成本: 需要大量的计算资源。 二、损失函数 DETR（Detection Transformer）使用了一种名为“Bipartite Matching Loss”的特殊损失函数，这是其与传统目标检测算法最显著的不同之一。\nBipartite Matching Loss 这个损失函数主要由两部分组成：分类损失（Classification Loss）和位置损失（Localization Loss）。\n分类损失: 通常使用交叉熵损失（Cross-Entropy Loss）来计算每个预测框的分类损失。\n位置损失: 用于计算预测框和实际框（Ground Truth）之间的位置偏差，通常使用L1损失或者其他形式的回归损失。\n匹配机制 在计算损失之前，需要将预测的目标框与实际的目标框进行匹配。这里使用了一种全局的二分匹配算法（Bipartite Matching Algorithm）。\n成本矩阵: 首先计算一个成本矩阵，其中每个元素表示一个预测框与一个实际框之间的“距离”。这个距离是由分类损失和位置损失共同决定的。\n最优匹配: 使用匈牙利算法（Hungarian Algorithm）或其他优化算法找到成本矩阵的最优匹配。\n损失计算: 只有匹配的预测框和实际框之间的损失会被计算并反向传播。未匹配的预测框会被认为是“背景”，其分类损失会被设置为与背景类的交叉熵损失。\n优点和缺点 优点 全局优化: 由于使用了全局匹配，模型能够在整个图像范围内进行优化，而不是局限于单个锚框或者预测框。\n简单直观: 不需要复杂的非极大值抑制（NMS）或者锚框策略。\n缺点 计算复杂性: 匹配算法通常需要O(N^3)的时间复杂性，这可能在有大量目标框的情况下成为瓶颈。\n对异常值敏感: 如果模型预测出大量错误的目标框，可能会影响匹配算法的性能。\n三、详细结构 backbone：主干网络，从图像中抽取特征 encoder：对主干网络输出的特征和位置编码相加后的序列进行进一步处理 decoder：将 decoder 的输出特征和 object queries 进行 cross attention，预测目标的类别和位置 prediction heads：进行 FFN 处理后输出目标类别和预测框 四、Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import torch from torch import nn from torchvision.models import resnet50 class DETR(nn.Module): def __init__(self, num_classes, hidden_dim, nheads,num_encoder_layers, num_decoder_layers): super().__init__() # We take only convolutional layers from ResNet-50 model self.backbone = nn.Sequential(*list(resnet50(pretrained=True).children())[:-2]) self.conv = nn.Conv2d(2048, hidden_dim, 1) self.transformer = nn.Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers) self.linear_class = nn.Linear(hidden_dim, num_classes + 1) self.linear_bbox = nn.Linear(hidden_dim, 4) self.query_pos = nn.Parameter(torch.rand(100, hidden_dim)) self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2)) self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2)) def forward(self, inputs): x = self.backbone(inputs) h = self.conv(x) H, W = h.shape[-2:] pos = torch.cat([ self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1), self.row_embed[:H].unsqueeze(1).repeat(1, W, 1), ], dim=-1).flatten(0, 1).unsqueeze(1) h = self.transformer(pos + h.flatten(2).permute(2, 0, 1), self.query_pos.unsqueeze(1)) return self.linear_class(h), self.linear_bbox(h).sigmoid() detr = DETR(num_classes=91, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6) detr.eval() inputs = torch.randn(1, 3, 800, 1200) logits, bboxes = detr(inputs) 五、Problems 1、为什么DETR不需要nms 每个 object query 代表一个预测目标，由于经过了 transformer 处理，使得每个 object query 能够获取其他 object query 的信息，尽量避免对相同目标进行预测。\n","date":"2025-05-20T16:05:50+08:00","permalink":"https://Nyotenn.github.io/Notes/p/detr/","title":"DETR"},{"content":"一、实验准备 1、模型架构 将FasterVLM应用在不同的VLM上，包括LLaVA-1.5（7B和13B）、LLaVA-NeXT-7B [33] for high-resolution image inputs、Video-LLaVA-7B [30] for video understanding。\n二、实验结果 1、FasterVLM with higher resolution LLaVA-NeXT包含了更多的visual tokens（2880个visual tokens），隐含了更多的冗余。通过95%的缩减率，FasterVLM达到了原先88.35%的表现。\n2、消融实验 随机裁剪 基于所有patches的注意力的均值裁剪 基于[CLS]注意力裁剪 其中，随机裁剪的效果并不差，进一步证明了VLMs中visual tokens的冗余性。\n三、代码分析 1、model_vqa.py 设置随机种子 加载模型 读取 questions 文件，将其分成不同 chunks，获取下标为 chunk_idx 的 chunk，并遍历其中的 question 拼接 image token 和 question 加载会话对象，添加 message 使用分隔符拼接所有 message，并将其返回作为 prompt 将 prompt 转为 tokens，并进行答案生成 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def eval_model(args): # 设置随机种子 # 加载模型 # 读取questions文件，将其分成不同chunks，获取下标为`chunk_idx`的chunk questions = [json.loads(q) for q in open(os.path.expanduser(args.question_file), \u0026#34;r\u0026#34;)] questions = get_chunk(questions, args.num_chunks, args.chunk_idx) question_bar = tqdm(questions) for line in question_bar: idx = line[\u0026#34;question_id\u0026#34;] image_file = line[\u0026#34;image\u0026#34;] qs = line[\u0026#34;text\u0026#34;] cur_prompt = qs # 拼接 `image token` 和 question if model.config.mm_use_im_start_end: qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \u0026#39;\\n\u0026#39; + qs else: qs = DEFAULT_IMAGE_TOKEN + \u0026#39;\\n\u0026#39; + qs # 加载会话对象，添加message conv = conv_templates[args.conv_mode].copy() conv.append_message(conv.roles[0], qs) conv.append_message(conv.roles[1], None) # 使用分隔符拼接所有message，并将其返回作为prompt prompt = conv.get_prompt() # 将prompt转为tokens，并进行答案生成 input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\u0026#39;pt\u0026#39;).unsqueeze(0).cuda() image = Image.open(os.path.join(args.image_folder, image_file)).convert(\u0026#39;RGB\u0026#39;) image_tensor = process_images([image], image_processor, model.config)[0] with torch.inference_mode(): output_ids, v_token_num, image_attns = model.generate( input_ids, images=image_tensor.unsqueeze(0).half().cuda(), image_sizes=[image.size], do_sample=True if args.temperature \u0026gt; 0 else False, temperature=args.temperature, top_p=args.top_p, num_beams=args.num_beams, # no_repeat_ngram_size=3, max_new_tokens=1024, use_cache=True) visual_token_nums.append(v_token_num) outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip() 2、builder.py 加载模型\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 # /model/builder.py import os import warnings import shutil from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig import torch from llava.model import * from llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\u0026#34;auto\u0026#34;, device=\u0026#34;cuda\u0026#34;, use_flash_attn=False, **kwargs): # 初始化 `kwargs` if \u0026#39;llava\u0026#39; in model_name.lower(): # Load LLaVA model if \u0026#39;lora\u0026#39; in model_name.lower() and model_base is not None: from llava.model.language_model.llava_llama import LlavaConfig lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path) tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False) print(\u0026#39;Loading LLaVA from base model...\u0026#39;) model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs) token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features if model.lm_head.weight.shape[0] != token_num: model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype)) model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype)) print(\u0026#39;Loading additional LLaVA weights...\u0026#39;) if os.path.exists(os.path.join(model_path, \u0026#39;non_lora_trainables.bin\u0026#39;)): non_lora_trainables = torch.load(os.path.join(model_path, \u0026#39;non_lora_trainables.bin\u0026#39;), map_location=\u0026#39;cpu\u0026#39;) else: # this is probably from HF Hub from huggingface_hub import hf_hub_download def load_from_hf(repo_id, filename, subfolder=None): cache_file = hf_hub_download( repo_id=repo_id, filename=filename, subfolder=subfolder) return torch.load(cache_file, map_location=\u0026#39;cpu\u0026#39;) non_lora_trainables = load_from_hf(model_path, \u0026#39;non_lora_trainables.bin\u0026#39;) non_lora_trainables = {(k[11:] if k.startswith(\u0026#39;base_model.\u0026#39;) else k): v for k, v in non_lora_trainables.items()} if any(k.startswith(\u0026#39;model.model.\u0026#39;) for k in non_lora_trainables): non_lora_trainables = {(k[6:] if k.startswith(\u0026#39;model.\u0026#39;) else k): v for k, v in non_lora_trainables.items()} model.load_state_dict(non_lora_trainables, strict=False) from peft import PeftModel print(\u0026#39;Loading LoRA weights...\u0026#39;) model = PeftModel.from_pretrained(model, model_path) print(\u0026#39;Merging LoRA weights...\u0026#39;) model = model.merge_and_unload() print(\u0026#39;Model is loaded...\u0026#39;) elif model_base is not None: # this may be mm projector only print(\u0026#39;Loading LLaVA from base model...\u0026#39;) if \u0026#39;mpt\u0026#39; in model_name.lower(): if not os.path.isfile(os.path.join(model_path, \u0026#39;configuration_mpt.py\u0026#39;)): shutil.copyfile(os.path.join(model_base, \u0026#39;configuration_mpt.py\u0026#39;), os.path.join(model_path, \u0026#39;configuration_mpt.py\u0026#39;)) tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True) cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True) model = LlavaMptForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs) else: tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False) cfg_pretrained = AutoConfig.from_pretrained(model_path) model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs) mm_projector_weights = torch.load(os.path.join(model_path, \u0026#39;mm_projector.bin\u0026#39;), map_location=\u0026#39;cpu\u0026#39;) mm_projector_weights = {k: v.to(torch.float16) for k, v in mm_projector_weights.items()} model.load_state_dict(mm_projector_weights, strict=False) else: if \u0026#39;mpt\u0026#39; in model_name.lower(): tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True) model = LlavaMptForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs) elif \u0026#39;mistral\u0026#39; in model_name.lower(): tokenizer = AutoTokenizer.from_pretrained(model_path) model = LlavaMistralForCausalLM.from_pretrained( model_path, low_cpu_mem_usage=True, **kwargs ) else: tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False) model = LlavaLlamaForCausalLM.from_pretrained( model_path, low_cpu_mem_usage=True, **kwargs ) else: # Load language model if model_base is not None: # PEFT model from peft import PeftModel tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False) model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs) print(f\u0026#34;Loading LoRA weights from {model_path}\u0026#34;) model = PeftModel.from_pretrained(model, model_path) print(f\u0026#34;Merging weights\u0026#34;) model = model.merge_and_unload() print(\u0026#39;Convert to FP16...\u0026#39;) model.to(torch.float16) else: use_fast = False if \u0026#39;mpt\u0026#39; in model_name.lower(): tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True) model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs) else: tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False) model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs) image_processor = None if \u0026#39;llava\u0026#39; in model_name.lower(): mm_use_im_start_end = getattr(model.config, \u0026#34;mm_use_im_start_end\u0026#34;, False) mm_use_im_patch_token = getattr(model.config, \u0026#34;mm_use_im_patch_token\u0026#34;, True) if mm_use_im_patch_token: tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True) if mm_use_im_start_end: tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True) model.resize_token_embeddings(len(tokenizer)) vision_tower = model.get_vision_tower() if not vision_tower.is_loaded: vision_tower.load_model(device_map=device_map) if device_map != \u0026#39;auto\u0026#39;: vision_tower.to(device=device_map, dtype=torch.float16) image_processor = vision_tower.image_processor if hasattr(model.config, \u0026#34;max_sequence_length\u0026#34;): context_len = model.config.max_sequence_length else: context_len = 2048 return tokenizer, model, image_processor, context_len 3、llava_llama.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 # /model/language_model/llava_llama.py from typing import List, Optional, Tuple, Union import torch import torch.nn as nn from transformers import AutoConfig, AutoModelForCausalLM, \\ LlamaConfig, LlamaModel, LlamaForCausalLM from transformers.modeling_outputs import CausalLMOutputWithPast from transformers.generation.utils import GenerateOutput from ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM class LlavaConfig(LlamaConfig): model_type = \u0026#34;llava_llama\u0026#34; class LlavaLlamaModel(LlavaMetaModel, LlamaModel): config_class = LlavaConfig # 初始化实例及其父类实例 def __init__(self, config: LlamaConfig): super(LlavaLlamaModel, self).__init__(config) class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM): config_class = LlavaConfig def __init__(self, config, visual_token_num=None): # 从LlamaForCausalLM开始，调用下一个__init__() super(LlamaForCausalLM, self).__init__(config) self.model = LlavaLlamaModel(config) self.pretraining_tp = config.pretraining_tp self.vocab_size = config.vocab_size self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False) # FasterVLM self.visual_token_num = visual_token_num # Initialize weights and apply final processing self.post_init() def get_model(self): return self.model # FasterVLM def get_visual_token_num(self): return self.visual_token_num def forward(...) -\u0026gt; Union[Tuple, CausalLMOutputWithPast]: if inputs_embeds is None: # 调用 `LlavaMetaForCausalLM` 的方法 (...) = self.prepare_inputs_labels_for_multimodal(...) return super().forward(...) @torch.no_grad() def generate(...) -\u0026gt; Union[GenerateOutput, torch.LongTensor]: position_ids = kwargs.pop(\u0026#34;position_ids\u0026#34;, None) attention_mask = kwargs.pop(\u0026#34;attention_mask\u0026#34;, None) if \u0026#34;inputs_embeds\u0026#34; in kwargs: raise NotImplementedError(\u0026#34;`inputs_embeds` is not supported\u0026#34;) if images is not None: (...) = self.prepare_inputs_labels_for_multimodal(...) else: # inputs_embeds = self.get_model().embed_tokens(inputs) v_token_num, cls_attn = 0, None return super().generate(...), v_token_num, cls_attn def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs): images = kwargs.pop(\u0026#34;images\u0026#34;, None) image_sizes = kwargs.pop(\u0026#34;image_sizes\u0026#34;, None) inputs = super().prepare_inputs_for_generation( input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs ) if images is not None: inputs[\u0026#39;images\u0026#39;] = images if image_sizes is not None: inputs[\u0026#39;image_sizes\u0026#39;] = image_sizes return inputs AutoConfig.register(\u0026#34;llava_llama\u0026#34;, LlavaConfig) AutoModelForCausalLM.register(LlavaConfig, LlavaLlamaForCausalLM) 3、mm_utils.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None): prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(\u0026#39;\u0026lt;image\u0026gt;\u0026#39;)] def insert_separator(X, sep): return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1] input_ids = [] offset = 0 if len(prompt_chunks) \u0026gt; 0 and len(prompt_chunks[0]) \u0026gt; 0 and prompt_chunks[0][0] == tokenizer.bos_token_id: offset = 1 input_ids.append(prompt_chunks[0][0]) for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)): input_ids.extend(x[offset:]) if return_tensors is not None: if return_tensors == \u0026#39;pt\u0026#39;: return torch.tensor(input_ids, dtype=torch.long) raise ValueError(f\u0026#39;Unsupported tensor type: {return_tensors}\u0026#39;) return input_ids 4、llava_arch.py （1）LlavaMetaModel 从 config 中获取参数构建 vision_tower\n1 2 3 4 5 6 7 8 9 10 11 def __init__(self, config): super(LlavaMetaModel, self).__init__(config) if hasattr(config, \u0026#34;mm_vision_tower\u0026#34;): self.vision_tower = build_vision_tower(config, delay_load=True) self.mm_projector = build_vision_projector(config) if \u0026#39;unpad\u0026#39; in getattr(config, \u0026#39;mm_patch_merge_type\u0026#39;, \u0026#39;\u0026#39;): self.image_newline = nn.Parameter( torch.empty(config.hidden_size, dtype=self.dtype) ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def initialize_vision_modules(self, model_args, fsdp=None): vision_tower = model_args.vision_tower mm_vision_select_layer = model_args.mm_vision_select_layer mm_vision_select_feature = model_args.mm_vision_select_feature pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter mm_patch_merge_type = model_args.mm_patch_merge_type self.config.mm_vision_tower = vision_tower if self.get_vision_tower() is None: vision_tower = build_vision_tower(model_args) if fsdp is not None and len(fsdp) \u0026gt; 0: self.vision_tower = [vision_tower] else: self.vision_tower = vision_tower else: if fsdp is not None and len(fsdp) \u0026gt; 0: vision_tower = self.vision_tower[0] else: vision_tower = self.vision_tower vision_tower.load_model() self.config.use_mm_proj = True self.config.mm_projector_type = getattr(model_args, \u0026#39;mm_projector_type\u0026#39;, \u0026#39;linear\u0026#39;) self.config.mm_hidden_size = vision_tower.hidden_size self.config.mm_vision_select_layer = mm_vision_select_layer self.config.mm_vision_select_feature = mm_vision_select_feature self.config.mm_patch_merge_type = mm_patch_merge_type if getattr(self, \u0026#39;mm_projector\u0026#39;, None) is None: self.mm_projector = build_vision_projector(self.config) if \u0026#39;unpad\u0026#39; in mm_patch_merge_type: embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype)) self.image_newline = nn.Parameter( torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std ) else: # In case it is frozen by LoRA for p in self.mm_projector.parameters(): p.requires_grad = True if pretrain_mm_mlp_adapter is not None: mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location=\u0026#39;cpu\u0026#39;) def get_w(weights, keyword): return {k.split(keyword + \u0026#39;.\u0026#39;)[1]: v for k, v in weights.items() if keyword in k} self.mm_projector.load_state_dict(get_w(mm_projector_weights, \u0026#39;mm_projector\u0026#39;)) （2）LlavaMetaForCausalLM get_vision_tower 函数返回 CLIPVisionTower 或 CLIPVisionTowerS2 实例。\n调用 vision_tower 获取 image_features 和 image_attentions image_features：(batch_size, sequence_length, hidden_size) image_attentions：(batch_size, num_heads, sequence_length) 在多个注意力头上取平均值，image_attentions：(batch_size, sequence_length) B = batch_size, N = sequence_length 从 image_attentions 中取出最大的 visual_token_num 个，token_indices 为其索引 对 token_indices 按升序排序 根据 token_indices 生成 mask 矩阵 构建一个 projector 映射 image_features 返回参数： image_features：图像特征，(batch_size, sequence_length, new_hidden_size) index_mask：掩码矩阵，(batch_size, sequence_length) image_attentions：注意力向量，(batch_size, sequence_length) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def encode_images(self, images): # 获取 [CLS] token 的Attention image_features, image_attentions = self.get_model().get_vision_tower()(images) # (B, N, C), (B, M, N) = (1, 576, 1024), (1, 16, 576) image_attentions = image_attentions.mean(dim=1) # (B, N) = (1, 576) B, N = image_features.shape[:2] visual_token_num = self.get_visual_token_num() # T # prune visual tokens by random scores # token_weights = torch.rand(B, N, device=image_features.device) # (B, N) # token_indices = torch.topk(token_weights, k=visual_token_num, dim=1)[1] # (B, T) # token_indices = torch.sort(token_indices, dim=1)[0] # (B, T) # prune visual tokens by attention scores token_indices = torch.topk(image_attentions, k=visual_token_num, dim=1)[1] # (B, T) token_indices = torch.sort(token_indices, dim=1)[0] # (B, T) # generate index mask index_mask = torch.zeros(B, N, dtype=torch.bool, device=image_features.device) # (B, N) index_mask.scatter_(1, token_indices, True) # (B, N) image_features = self.get_model().mm_projector(image_features) # (B, N, D) return image_features, index_mask, image_attentions 调用 encode_images 处理图像，并用 index_mask 过滤 image_feature 将 image_feature 拼接为 image_features 遍历 input_ids： 计算cur_input_ids中值等于IMAGE_TOKEN_INDEX的数量，这个值表示在当前输入中有多少个图像标记。 如果当前批次没有图像标记，则直接获取对应位置的图像特征（尽管这里实际上并不使用这些特征，因为cur_image_features[0:0]是一个空张量），并生成当前输入ID的嵌入。 将生成的嵌入和标签添加到新的嵌入和标签列表中，并增加图像索引计数器cur_image_idx，然后跳过后续逻辑继续下一个循环。 找出所有图像标记的位置，并创建一个包含这些位置的列表image_token_indices，为了方便处理，在列表开头和结尾分别添加了-1和输入长度作为边界。 根据图像标记的位置分割出不包含图像的输入ID片段和对应的标签。 获取每个分割部分的大小，用于后续拆分嵌入。 将所有不含图像的输入ID拼接在一起后进行嵌入，再根据之前计算的大小将其拆分成多个部分。 循环合并文本部分的嵌入和图像的嵌入，以及相应的标签。对于图像部分，使用IGNORE_INDEX填充标签。 将所有嵌入转换为同一设备（可能是GPU），然后拼接起来形成最终的嵌入和标签，最后将它们添加到新嵌入和新标签列表中，供后续使用。 截断嵌入和标签，使其不超过最大长度。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def prepare_inputs_labels_for_multimodal(...): vision_tower = self.get_vision_tower() if type(images) is list or images.ndim == 5: ... else: image_features, index_masks, image_attns = self.encode_images(images) new_image_features = [] for image_feature, index_mask in zip(image_features, index_masks): image_feature = image_feature[index_mask] new_image_features.append(image_feature) image_features = torch.stack(new_image_features, dim=0) new_input_embeds = [] new_labels = [] cur_image_idx = 0 for batch_idx, cur_input_ids in enumerate(input_ids): num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum() if num_images == 0: cur_image_features = image_features[cur_image_idx] cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids) cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0) new_input_embeds.append(cur_input_embeds) new_labels.append(labels[batch_idx]) cur_image_idx += 1 continue image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]] cur_input_ids_noim = [] cur_labels = labels[batch_idx] cur_labels_noim = [] for i in range(len(image_token_indices) - 1): cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]]) cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]]) split_sizes = [x.shape[0] for x in cur_labels_noim] cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim)) cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0) cur_new_input_embeds = [] cur_new_labels = [] for i in range(num_images + 1): cur_new_input_embeds.append(cur_input_embeds_no_im[i]) cur_new_labels.append(cur_labels_noim[i]) if i \u0026lt; num_images: cur_image_features = image_features[cur_image_idx] cur_image_idx += 1 cur_new_input_embeds.append(cur_image_features) cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype)) cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds] cur_new_input_embeds = torch.cat(cur_new_input_embeds) cur_new_labels = torch.cat(cur_new_labels) new_input_embeds.append(cur_new_input_embeds) new_labels.append(cur_new_labels) # Truncate sequences to max length as image embeddings can make the sequence longer tokenizer_model_max_length = getattr(self.config, \u0026#39;tokenizer_model_max_length\u0026#39;, None) if tokenizer_model_max_length is not None: new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds] new_labels = [x[:tokenizer_model_max_length] for x in new_labels] # Combine them max_len = max(x.shape[0] for x in new_input_embeds) batch_size = len(new_input_embeds) new_input_embeds_padded = [] new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device) attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device) position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device) for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)): cur_len = cur_new_embed.shape[0] if getattr(self.config, \u0026#39;tokenizer_padding_side\u0026#39;, \u0026#39;right\u0026#39;) == \u0026#34;left\u0026#34;: new_input_embeds_padded.append(torch.cat(( torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device), cur_new_embed ), dim=0)) if cur_len \u0026gt; 0: new_labels_padded[i, -cur_len:] = cur_new_labels attention_mask[i, -cur_len:] = True position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device) else: new_input_embeds_padded.append(torch.cat(( cur_new_embed, torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device) ), dim=0)) if cur_len \u0026gt; 0: new_labels_padded[i, :cur_len] = cur_new_labels attention_mask[i, :cur_len] = True position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device) new_input_embeds = torch.stack(new_input_embeds_padded, dim=0) if _labels is None: new_labels = None else: new_labels = new_labels_padded if _attention_mask is None: attention_mask = None else: attention_mask = attention_mask.to(dtype=_attention_mask.dtype) if _position_ids is None: position_ids = None return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels, image_features[0].shape[0], image_attns 5、clip_encoder.py （1）CLIPVisionTower 使用 CLIPVisionModel 加载 vision_tower\n使用 vision_tower 处理图像，得到 image_forward_out\n其中包含属性：\nhidden_states：(layer_num, batch_size, sequence_length + 1, hidden_size) attentions：(layer_num, batch_size, num_heads, sequence_length + 1, sequence_length + 1) 调用 feature_select 函数处理 image_forward_out\n选择对应层的 hidden_states 和 attentions 如果 self.select_feature == \u0026lsquo;patch\u0026rsquo;，则： image_features 过滤掉 [CLS] Token 部分，留下剩余部分，(batch_size, sequence_length, hidden_size) image_attentions 取 [CLS] Token 对其他部分的注意力，(batch_size, num_heads, sequence_length) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class CLIPVisionTower(nn.Module): def __init__(self, vision_tower, args, delay_load=False): super().__init__() self.is_loaded = False self.vision_tower_name = vision_tower self.select_layer = args.mm_vision_select_layer self.select_feature = getattr(args, \u0026#39;mm_vision_select_feature\u0026#39;, \u0026#39;patch\u0026#39;) if not delay_load: self.load_model() elif getattr(args, \u0026#39;unfreeze_mm_vision_tower\u0026#39;, False): self.load_model() else: self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name) def load_model(self, device_map=None): if self.is_loaded: print(\u0026#39;{} is already loaded, `load_model` called again, skipping.\u0026#39;.format(self.vision_tower_name)) return self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name) self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map) self.vision_tower.requires_grad_(False) self.is_loaded = True # FasterVLM # 如果 self.select_feature 为 \u0026#39;patch\u0026#39;，则去除 CLS token 并调整注意力权重。如果 self.select_feature 为 \u0026#39;cls_patch\u0026#39;，则不作任何处理。 def feature_select(self, image_forward_outs): image_features = image_forward_outs.hidden_states[self.select_layer] image_attentions = image_forward_outs.attentions[self.select_layer] if self.select_feature == \u0026#39;patch\u0026#39;: image_features = image_features[:, 1:] image_attentions = image_attentions[:, :, 0, 1:] elif self.select_feature == \u0026#39;cls_patch\u0026#39;: image_features = image_features image_attentions = image_attentions else: raise ValueError(f\u0026#39;Unexpected select feature: {self.select_feature}\u0026#39;) return image_features, image_attentions # FasterVLM @torch.no_grad() def forward(self, images): if type(images) is list: image_features, image_attentions = [], [] for image in images: image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_attentions=True, output_hidden_states=True) image_feature, image_attention = self.feature_select(image_forward_out) image_features.append(image_feature.to(image.dtype)) image_attentions.append(image_attention.to(image.dtype)) else: image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_attentions=True, output_hidden_states=True) image_features, image_attentions = self.feature_select(image_forward_outs) image_features, image_attentions = image_features.to(images.dtype), image_attentions.to(images.dtype) return image_features, image_attentions @property def dummy_feature(self): return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype) @property def dtype(self): return self.vision_tower.dtype @property def device(self): return self.vision_tower.device @property def config(self): if self.is_loaded: return self.vision_tower.config else: return self.cfg_only @property def hidden_size(self): return self.config.hidden_size @property def num_patches_per_side(self): return self.config.image_size // self.config.patch_size @property def num_patches(self): return (self.config.image_size // self.config.patch_size) ** 2 ","date":"2025-05-20T16:05:50+08:00","permalink":"https://Nyotenn.github.io/Notes/p/fastervlm/","title":"FasterVLM"},{"content":"Learning to Learn from Noisy Labeled Data 一、Introduction Training on noisy labeled datasets causes performance degradation because DNNs can easily overfifit to the label noise. To overcome this problem, we propose a noise-tolerant training algorithm, where a meta-learning update is performed prior to conventional gradient update.\n目的\n论文的目的是提出一种基于元学习的方法，可以从带有噪声标签的数据中学习鲁棒的特征表示和分类模型，而不需要任何人工干预或清洗数据。\n方法\n论文的方法是在每个小批量数据上，生成一些合成的噪声标签，然后用这些噪声标签对模型进行梯度更新，同时用一个教师模型来指导学生模型，使得学生模型在不同的噪声标签下都能给出一致的预测结果。\n创新点\n论文的创新点是提出了一个元学习目标，即训练模型使得它在经过一次梯度更新后不会过拟合噪声标签，而是能够捕捉到数据的真实特征。论文还提出了一个双重对比学习的框架，利用正负样本对来增强模型的判别能力。\n核心思想\n论文的核心思想是：a noise-tolerant model should be able to consistently learn the underlying knowledge from data despite different label noise.\n1、相似工作 大多数工作是拟合一个模型，来近似噪声和真实标签的分布。\n但这种方式通常需要纯净的标签集合，并且如果噪声分布复杂，很难有效地进行拟合。\n二、Method 考虑有训练集：$D = {(x_1, y_1 ), \u0026hellip;,(x_n, y_n)}$\n其中，$x_i$ 代表第 i 个样本，$y_i \\in {0, 1}^c$ 是 c 个类别上相应的带噪声的标签\nconsider a mini-batch of data (X, Y) sampled from the training set, where $X = {x_1, \u0026hellip;, x_k}$ are k samples, and\n$Y = {y_1, \u0026hellip;,y_k}$ are the corresponding noisy labels\nrandomly select ρ samples out of the mini-batch of k samples, randomly select a neighbor $x_j$ from its top 10 nearest neighbors, use the neighbor’s label $y_j$ to replace the label for $x_i, \\hat{y_i}^m = y_i$\nrepeat the above procedure M times to generate M mini-batches of synthetic noisy label\n$\\theta_m\u0026rsquo; = \\theta - \\alpha \\nabla_\\theta L_c(X, \\hat{Y}m, \\theta)$​，其中 $L_c = -\\frac{1}{n}\\sum{i=1}^n y_i \\cdot \\log(f(x_i, \\theta))$\n","date":"2025-05-20T16:05:30+08:00","permalink":"https://Nyotenn.github.io/Notes/p/learning-to-learn-from-noisy-labeled-data/","title":"Learning to Learn from Noisy Labeled Data"},{"content":"ViT 一、Introduction 在计算机视觉领域，Attention 机制要么和卷积网络一起使用，要么只是替代卷积网络的部分并保持总体结构不变。\nViT 的提出，表明对卷积网络的依赖是不必要的，可以直接将图片打成数个 patches，将一个纯粹的 Transformer 应用于这些 patches 之上，仍然能在图像分类任务上有良好表现。\n二、Related Works Image Transformer 仅在邻域而非全局上采用 self-attention ，这样的多头注意力机制取代了卷积操作。 Sparse Transformer 选择稀疏 Transformer 模型中的 attention 矩阵。 三、Coding image_size：图片尺寸，int或者tuple皆可，例如224，或者(224, 224)，长宽不一定要一样大 path_size：分块path尺寸，int或tuple，默认为16，需要确保image_size能被path_size*整除 num_classes：分类数，int dim：Transformer隐层维度，对于Base来说是768，Large为1024 depth：Transformer个数，Base为12 head：多头的个数，Base=12 mlp_dim：Transformer中的FeedForward中第一个线性层升维后的维度，默认为768*4，先升维4倍再降维回去 pool：默认\u0026rsquo;cls，选取CLS token作为输出，可选\u0026rsquo;mean\u0026rsquo;，在patch维度做平均池化 channel：图片输入的特征维度，RGB图像为3，灰度图为1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class ViT(nn.Module): def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = \u0026#39;cls\u0026#39;, channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.): super().__init__() image_height, image_width = pair(image_size) # 在这个项目中没有限定图片的尺寸 patch_height, patch_width = pair(patch_size) # 默认为16 assert image_height % patch_height == 0 and image_width % patch_width == 0, \u0026#39;Image dimensions must be divisible by the patch size.\u0026#39; # num patches -\u0026gt; (224 / 16) = 14, 14 * 14 = 196 num_patches = (image_height // patch_height) * (image_width // patch_width) # path dim -\u0026gt; 3 * 16 * 16 = 768，和Bert-base一致 patch_dim = channels * patch_height * patch_width assert pool in {\u0026#39;cls\u0026#39;, \u0026#39;mean\u0026#39;}, \u0026#39;pool type must be either cls (cls token) or mean (mean pooling)\u0026#39; # 输出选cls token还是做平均池化 # 步骤一：图像分块与映射。首先将图片分块，然后接一个线性层做映射 self.to_patch_embedding = nn.Sequential( Rearrange(\u0026#39;b c (h p1) (w p2) -\u0026gt; b (h w) (p1 p2 c)\u0026#39;, p1 = patch_height, p2 = patch_width), nn.Linear(patch_dim, dim), ) # pos_embedding：位置编码；cls_token：在序列最前面插入一个cls token作为分类输出 self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim)) self.cls_token = nn.Parameter(torch.randn(1, 1, dim)) self.dropout = nn.Dropout(emb_dropout) # 步骤二：Transformer Encoder结构来提特征 self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout) self.pool = pool self.to_latent = nn.Identity() # 这一\t步上面都没做 # 线性层输出 self.mlp_head = nn.Sequential( nn.LayerNorm(dim), nn.Linear(dim, num_classes) ) def forward(self, img): ... ","date":"2025-05-20T16:04:50+08:00","permalink":"https://Nyotenn.github.io/Notes/p/vit/","title":"ViT"},{"content":"CLIP 代码 1、CLIPVisionModel 调用 CLIPVisionTransformer\n1 2 3 4 5 6 7 8 9 class CLIPVisionModel(CLIPPreTrainedModel): def __init__(self, config: CLIPVisionConfig): super().__init__(config) self.vision_model = CLIPVisionTransformer(config) self.post_init() def forward(...): return self.vision_model(...) 2、CLIPVisionTransformer 实例化 CLIPVisionEmbeddings 作为 embeddings 层，处理 pixel_values 得到 hidden_states 使用 layernorm 进行处理 CLIPEncoder 处理得到 encoder_outputs。 last_hidden_state：最后一层 layer 的 hidden_states 输出，(batch_size, sequence_length, hidden_size) hidden_states：所有 layer 的 hidden_states 输出 attentions：所有 layer 的 attn_weights 输出，(batch_size, num_heads, sequence_length, sequence_length) 取 [CLS] Token 作为 pooled_output 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class CLIPVisionTransformer(nn.Module): def __init__(self, config: CLIPVisionConfig): super().__init__() self.config = config embed_dim = config.hidden_size self.embeddings = CLIPVisionEmbeddings(config) self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps) self.encoder = CLIPEncoder(config) self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps) def forward(...): hidden_states = self.embeddings(pixel_values) hidden_states = self.pre_layrnorm(hidden_states) encoder_outputs = self.encoder( inputs_embeds=hidden_states, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, ) last_hidden_state = encoder_outputs[0] pooled_output = last_hidden_state[:, 0, :] pooled_output = self.post_layernorm(pooled_output) if not return_dict: return (last_hidden_state, pooled_output) + encoder_outputs[1:] return BaseModelOutputWithPooling( last_hidden_state=last_hidden_state, pooler_output=pooled_output, hidden_states=encoder_outputs.hidden_states, attentions=encoder_outputs.attentions, ) 3、CLIPVisionEmbeddings 初始化 [CLS] embedding、patch_embedding 和 position_embedding。其中，patch_embedding 是一个二维卷积 注册一个缓冲区，用于存储位置编码 将 pixel_values 经 patch_embedding 处理，得到 patch_embeds，形状为 (batch_size, self.embed_dim, H_out, W_out) 先使用 flatten(2) 将最后两个维度展平，变为 (batch_size, self.embed_dim, height * width)，再使用 transpose(1, 2) 交换第1和第2维度，最终得到 (batch_size, height * width, self.embed_dim) 使用 expand(batch_size, 1, -1) 将 class_embedding 扩展为 (batch_size, 1, self.embed_dim) 连接 class_embeds, patch_embeds，添加位置编码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class CLIPVisionEmbeddings(nn.Module): def __init__(self, config: CLIPVisionConfig): super().__init__() self.class_embedding = nn.Parameter(torch.randn(self.embed_dim)) self.patch_embedding = nn.Conv2d( in_channels=config.num_channels, out_channels=self.embed_dim, kernel_size=self.patch_size, stride=self.patch_size, bias=False, ) self.num_patches = (self.image_size // self.patch_size) ** 2 self.num_positions = self.num_patches + 1 self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim) self.register_buffer(\u0026#34;position_ids\u0026#34;, torch.arange(self.num_positions).expand((1, -1)), persistent=False) def forward(self, pixel_values: torch.FloatTensor) -\u0026gt; torch.Tensor: batch_size = pixel_values.shape[0] target_dtype = self.patch_embedding.weight.dtype patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype)) # shape = [*, width, grid, grid] patch_embeds = patch_embeds.flatten(2).transpose(1, 2) class_embeds = self.class_embedding.expand(batch_size, 1, -1) embeddings = torch.cat([class_embeds, patch_embeds], dim=1) embeddings = embeddings + self.position_embedding(self.position_ids) return embeddings 4、CLIPEncoder 初始化 num_hidden_layers 个 CLIPEncoderLayer 作为 layers 遍历经过 layers 处理，得到 layer_outputs 返回 hidden_states、encoder_states、all_attentions hidden_states：最后一层 layer 的 hidden_states 输出 encoder_states：所有 layer 的 hidden_states 输出 all_attentions：所有 layer 的 attn_weights 输出 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class CLIPEncoder(nn.Module): def __init__(self, config: CLIPConfig): super().__init__() self.config = config self.layers = nn.ModuleList([CLIPEncoderLayer(config) for _ in range(config.num_hidden_layers)]) self.gradient_checkpointing = False def forward(...): encoder_states = () if output_hidden_states else None all_attentions = () if output_attentions else None hidden_states = inputs_embeds for idx, encoder_layer in enumerate(self.layers): layer_outputs = encoder_layer( hidden_states, attention_mask, causal_attention_mask, output_attentions=output_attentions, ) hidden_states = layer_outputs[0] if output_attentions: all_attentions = all_attentions + (layer_outputs[1],) if output_hidden_states: encoder_states = encoder_states + (hidden_states,) if not return_dict: return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None) return BaseModelOutput( last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions ) 5、CLIPEncoderLayer CLIPEncoderLayer 结构如上图所示，其中经过 self_attn 处理后得到 hidden_states 和 attn_weights。\nhidden_states 经后续处理后和 attn_weights 拼接，作为输出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class CLIPEncoderLayer(nn.Module): def __init__(self, config: CLIPConfig): super().__init__() self.embed_dim = config.hidden_size self.self_attn = CLIPAttention(config) self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps) self.mlp = CLIPMLP(config) self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps) def forward(): residual = hidden_states hidden_states = self.layer_norm1(hidden_states) hidden_states, attn_weights = self.self_attn( hidden_states=hidden_states, attention_mask=attention_mask, causal_attention_mask=causal_attention_mask, output_attentions=output_attentions, ) hidden_states = residual + hidden_states residual = hidden_states hidden_states = self.layer_norm2(hidden_states) hidden_states = self.mlp(hidden_states) hidden_states = residual + hidden_states outputs = (hidden_states,) if output_attentions: outputs += (attn_weights,) return outputs 6、CLIPAttention hidden_states 的输入形状为(batch_size, 1 + height * width, embed_dim)\nattn_output 的输出形状为(batch_size, 1 + height * width, embed_dim)\nattn_weights_reshaped 的输出形状为(batch_size * num_heads, 1 + height * width, src_len)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class CLIPAttention(nn.Module): def __init__(self, config): super().__init__() self.config = config self.embed_dim = config.hidden_size self.num_heads = config.num_attention_heads self.head_dim = self.embed_dim // self.num_heads if self.head_dim * self.num_heads != self.embed_dim: raise ValueError( f\u0026#34;embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\u0026#34; f\u0026#34; {self.num_heads}).\u0026#34; ) self.scale = self.head_dim**-0.5 self.dropout = config.attention_dropout self.k_proj = nn.Linear(self.embed_dim, self.embed_dim) self.v_proj = nn.Linear(self.embed_dim, self.embed_dim) self.q_proj = nn.Linear(self.embed_dim, self.embed_dim) self.out_proj = nn.Linear(self.embed_dim, self.embed_dim) def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int): return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous() def forward( self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, causal_attention_mask: Optional[torch.Tensor] = None, output_attentions: Optional[bool] = False, ) -\u0026gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]: \u0026#34;\u0026#34;\u0026#34;Input shape: Batch x Time x Channel\u0026#34;\u0026#34;\u0026#34; bsz, tgt_len, embed_dim = hidden_states.size() # get query proj query_states = self.q_proj(hidden_states) * self.scale key_states = self._shape(self.k_proj(hidden_states), -1, bsz) value_states = self._shape(self.v_proj(hidden_states), -1, bsz) proj_shape = (bsz * self.num_heads, -1, self.head_dim) query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape) key_states = key_states.view(*proj_shape) value_states = value_states.view(*proj_shape) src_len = key_states.size(1) attn_weights = torch.bmm(query_states, key_states.transpose(1, 2)) if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len): raise ValueError( f\u0026#34;Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\u0026#34; f\u0026#34; {attn_weights.size()}\u0026#34; ) # apply the causal_attention_mask first if causal_attention_mask is not None: if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len): raise ValueError( f\u0026#34;Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\u0026#34; f\u0026#34; {causal_attention_mask.size()}\u0026#34; ) attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len) if attention_mask is not None: if attention_mask.size() != (bsz, 1, tgt_len, src_len): raise ValueError( f\u0026#34;Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\u0026#34; ) attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len) attn_weights = nn.functional.softmax(attn_weights, dim=-1) if output_attentions: # this operation is a bit akward, but it\u0026#39;s required to # make sure that attn_weights keeps its gradient. # In order to do so, attn_weights have to reshaped # twice and have to be reused in the following attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len) else: attn_weights_reshaped = None attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training) attn_output = torch.bmm(attn_probs, value_states) if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim): raise ValueError( f\u0026#34;`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\u0026#34; f\u0026#34; {attn_output.size()}\u0026#34; ) attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim) attn_output = attn_output.transpose(1, 2) attn_output = attn_output.reshape(bsz, tgt_len, embed_dim) attn_output = self.out_proj(attn_output) return attn_output, attn_weights_reshaped ","date":"2025-05-20T10:36:50+08:00","permalink":"https://Nyotenn.github.io/Notes/p/clip/","title":"CLIP"},{"content":"Moco 一、Introduction We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder.\n1、Present situation 近些年，无监督学习在 nlp 领域获得了巨大的成果，但在 cv 领域仍和监督学习存在较大差距。\n作者认为，这种差距在于它们属于不同的信号空间。\n语言任务是在离散的信号空间，可以简单地构造出字典，这是用于无监督学习的基础。而图片任务是在连续的、高维的空间中，它更关注如何构建字典。\n2、Related works 对比学习\n一部分很有希望的研究是基于对比学习的。这些研究大体可以总结为以下方法：\n建立一个字典 字典的 key 是从样本中采样，然后经过 encoder 编码而来的 训练 encoder 使得 query 编码和它所匹配的样本 key 接近，而和其他 key 远离 3、Moco 和以往的对比学习不同的是，Moco用一个队列作为存储 key 的字典，当队列满后，新的 mini-batch 进来，旧的 mini-batch 出去。\n除此之外，由于字典规模大，encoder要学习的参数多，导致不能采用常规的 BP 方式进行学习，moco 采用了动量下降的方式进行学习，这一部分动量来自于 query encoder。\n1、training 上图列举了三种对比损失机制：\n第一种使用 BP 算法进行端到端的更新 第二种是从 memory bank 中采样 key moco 采用动量更新的 encoder 来产生 key，并维护一个队列来存储 key 二、Coding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # f_q, f_k: encoder networks for query and key # queue: dictionary as a queue of K keys (CxK) # m: momentum # t: temperature f_k.params = f_q.params # initialize for x in loader: # load a minibatch x with N samples x_q = aug(x) # a randomly augmented version x_k = aug(x) # another randomly augmented version q = f_q.forward(x_q) # queries: NxC k = f_k.forward(x_k) # keys: NxC k = k.detach() # no gradient to keys # positive logits: Nx1 l_pos = bmm(q.view(N,1,C), k.view(N,C,1)) # negative logits: NxK l_neg = mm(q.view(N,C), queue.view(C,K)) # logits: Nx(1+K) logits = cat([l_pos, l_neg], dim=1) # contrastive loss, Eqn.(1) labels = zeros(N) # positives are the 0-th loss = CrossEntropyLoss(logits/t, labels) # SGD update: query network loss.backward() update(f_q.params) # momentum update: key network f_k.params = m*f_k.params+(1-m)*f_q.params # update dictionary enqueue(queue, k) # enqueue the current minibatch dequeue(queue) # dequeue the earliest minibatch 三、Problems 1、Batch Normalization 正如标准的 resnet 一样，$f_q$ 和 $f_k$ 都存在 BN 操作，而 BN 会计算样本的方差和均值，这些被称为泄露信息。\n通过这些泄露的信息，编码器能够很容易找到正样本，而非去学习一个好的模型。\nBN操作\n计算均值和方差：对于每一个小批量（batch）的数据，计算其特征的均值和方差。 $$ μ = \\frac{1}{m} \\sum_{i=1}^{m} x_i, \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu)^2 $$ 归一化：使用计算出的均值和方差来归一化每个数据点。 $$ \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} $$ 其中，$\\epsilon$ 是一个非常小的数（例如 $1e-7$），用于防止分母为零。\n缩放和平移：归一化后的数据会被缩放（scale）和平移（shift）。 $$ y_i = \\gamma \\hat{x}_i + \\beta $$ 其中，$\\gamma$ 和 $\\beta$ 是可学习的参数。\n原理\n由于最后一步加上了缩放和平移，并交由神经网络进行学习，神经网络因此可根据 BN 操作的效益来决定缩放平移值的大小。\n如果归一化操作效果不好，则可以通过更改缩放平移值来抵消部分归一化的操作。\n2、Why dictionary a larger dictionary may better sample the underlying continuous, high dimensional visual space, while the keys in the dictionary should be represented by the same or similar encoder so that their comparisons to the query are consistent\n使用一个队列可以让字典的规模很大，但同时也让 encoder 变得难以训练。因为梯度下降会将梯度冒泡到全部样本。\n传统的一个解决方法是直接将 query encoder 作为 key encoder。但实际上，这种方法表现并不好，作者认为，是因为 encoder 变化的太快，导致字典中的 key 丧失了一致性。\n因此，moco 提出了动量更新的方式： $$ \\theta_k \\leftarrow m\\theta_k + (1-m)\\theta_q $$ 其中，$m \\in [0,1)$ 是动量系数，只有 $\\theta_q$ 是需要进行 BP 的。\n尽管队列中的 key 可能是不同编码器产生的，这些编码器的差异也很小（如取 m = 0.999）\n","date":"2025-05-20T10:36:50+08:00","permalink":"https://Nyotenn.github.io/Notes/p/moco/","title":"Moco"}]